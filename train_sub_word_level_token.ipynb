{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from new_transformer import Transformer\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "import tqdm\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPE_Tokenizer:\n",
    "    def _init_(self):\n",
    "        self.vocab = set()\n",
    "        self.token_to_index = {}\n",
    "        self.index_to_token = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def get_stats(vocab):\n",
    "        pairs = {}\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols)-1):\n",
    "                pair = (symbols[i], symbols[i+1])\n",
    "                if pair in pairs:\n",
    "                    pairs[pair] += freq\n",
    "                else:\n",
    "                    pairs[pair] = freq\n",
    "        return pairs\n",
    "\n",
    "    @staticmethod\n",
    "    def merge_vocab(pair, v_in):\n",
    "        v_out = {}\n",
    "        bigram = ' '.join(pair)\n",
    "        replacement = ''.join(pair)\n",
    "        for word in v_in:\n",
    "            w_out = word.replace(bigram, replacement)\n",
    "            v_out[w_out] = v_in[word]\n",
    "        return v_out\n",
    "\n",
    "    @staticmethod\n",
    "    def get_vocab(text):\n",
    "        vocab = {}\n",
    "        for word in text.split():\n",
    "            word = ' '.join(list(word)) +  ' </w>'\n",
    "            if word in vocab:\n",
    "                vocab[word] += 1\n",
    "            else:\n",
    "                vocab[word] = 1\n",
    "        return vocab\n",
    "\n",
    "    def train_until(self, text, vocab_size):\n",
    "        vocab = self.get_vocab(text)\n",
    "        self.vocab = set(word for word in vocab for word in word.split())\n",
    "        # Check if the initial vocabulary size is less than the desired size\n",
    "        while len(self.vocab) < vocab_size:\n",
    "            pairs = self.get_stats(vocab)\n",
    "            if not pairs:\n",
    "                break\n",
    "            best = max(pairs, key=pairs.get)\n",
    "            vocab = self.merge_vocab(best, vocab)\n",
    "            self.vocab = set(word for word in vocab for word in word.split())\n",
    "\n",
    "        # Add special tokens\n",
    "        self.vocab.add('</u>')\n",
    "        self.vocab.add('[]')\n",
    "\n",
    "        self.build_index()\n",
    "\n",
    "    def train(self, text, num_merges):\n",
    "            vocab = self.get_vocab(text)\n",
    "            for i in range(num_merges):\n",
    "                pairs = self.get_stats(vocab)\n",
    "                if not pairs:\n",
    "                    break\n",
    "                best = max(pairs, key=pairs.get)\n",
    "                vocab = self.merge_vocab(best, vocab)\n",
    "\n",
    "            self.vocab = set(word for word in vocab for word in word.split())\n",
    "\n",
    "            # Add special tokens\n",
    "            self.vocab.add('</u>')\n",
    "\n",
    "            self.build_index()\n",
    "\n",
    "    def build_index(self):\n",
    "        # existing code\n",
    "        self.token_to_index = {token: index for index, token in enumerate(self.vocab)}\n",
    "        self.index_to_token = {index: token for token, index in self.token_to_index.items()}\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        tokens = []\n",
    "        for word in text.split():\n",
    "            subwords = self.get_subwords(word + '</w>')\n",
    "            tokens.extend(self.token_to_index.get(sw, self.token_to_index['</u>']) for sw in subwords)\n",
    "        return tokens\n",
    "\n",
    "    def get_subwords(self, word):\n",
    "        subwords = []\n",
    "        while word:\n",
    "            subword = self.find_longest_subword(word)\n",
    "            if subword is None:\n",
    "                subwords.append('</u>')\n",
    "                break\n",
    "            subwords.append(subword)\n",
    "            word = word[len(subword):]\n",
    "        return subwords\n",
    "\n",
    "    def find_longest_subword(self, word):\n",
    "        for i in range(len(word), 0, -1):\n",
    "            if word[:i] in self.vocab:\n",
    "                return word[:i]\n",
    "        return None\n",
    "\n",
    "    def detokenize(self, token_ids):\n",
    "        words = []\n",
    "        current_word = ''\n",
    "        for token_id in token_ids:\n",
    "            token = self.index_to_token.get(token_id, '</u>')\n",
    "            if token == '</w>':\n",
    "                words.append(current_word)\n",
    "                current_word = ''\n",
    "            else:\n",
    "                current_word += token\n",
    "        words.append(current_word)\n",
    "        return ' '.join(words).replace('</w>', ' ')\n",
    "    \n",
    "    def add_special_tokens(self, special_tokens):\n",
    "        for token in special_tokens:\n",
    "            self.vocab.add(token)\n",
    "        self.build_index()\n",
    "\n",
    "    def save_vocab(self, file_path):\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(self.token_to_index, f)\n",
    "\n",
    "    def load_vocab(self, file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            self.token_to_index = json.load(f)\n",
    "            self.index_to_token = {int(index): token for token, index in self.token_to_index.items()}\n",
    "            self.vocab = set(self.token_to_index.keys())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.token_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utilisateur/createch/IA/A5/wite_me_a_poeme/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text length: 31353581\n",
      "Training...\n",
      "Vocab: {'lorin': 0, 'reu': 1, 'oreti': 2, 'renou</w>': 3, 'lone</w>': 4, 'Y': 5, 'aril': 6, 'lut</w>': 7, 'apou': 8, 'nor</w>': 9, 'aloi': 10, 'rentime</w>': 11, 'E': 12, 'poin': 13, 'eron</w>': 14, 'po</w>': 15, 'aloi</w>': 16, 'arerent</w>': 17, 'noron': 18, 'arer': 19, 'sing</w>': 20, 'erev': 21, 'eto</w>': 22, 'ronon': 23, 'poing</w>': 24, 'dily</w>': 25, 'loro': 26, 'eroi</w>': 27, 'orom': 28, 'apori': 29, 'ili</w>': 30, 'x</w>': 31, 'f</w>': 32, 'enti': 33, 'orero': 34, 'esin</w>': 35, 'elor</w>': 36, 'lorer</w>': 37, 'aporous</w>': 38, 'ating</w>': 39, 'oreli': 40, 'areron': 41, 'ronon</w>': 42, 'orec': 43, 'etis</w>': 44, 'anon': 45, 'lin': 46, 'ois</w>': 47, 'ald</w>': 48, 'ti</w>': 49, 'etit</w>': 50, 'arom': 51, 'lilu': 52, 're</w>': 53, 'wily</w>': 54, 'loin': 55, 'rori': 56, 'stilit</w>': 57, 'elous</w>': 58, 'entis</w>': 59, 'erois</w>': 60, 'oun</w>': 61, 'sili': 62, 'rech': 63, 'atil</w>': 64, 'eren': 65, 'estily</w>': 66, 'atil': 67, 'enoit</w>': 68, 'atiles</w>': 69, 'ari': 70, 'n': 71, 'sto</w>': 72, 'rorent</w>': 73, 'enof</w>': 74, 'eting</w>': 75, 'oreno': 76, 'arestin': 77, 'ero</w>': 78, 'estim': 79, 'arin': 80, 'astily</w>': 81, '8': 82, 'lil</w>': 83, 'elon': 84, 'i</w>': 85, 'arou': 86, 'lor': 87, 'ereli': 88, 'rer': 89, 'au': 90, 'erou</w>': 91, 'relon': 92, 'lome</w>': 93, 'poinon</w>': 94, 'wi': 95, 'ili': 96, 'wiles</w>': 97, 'ecom': 98, 'til': 99, 'ast</w>': 100, 'sily</w>': 101, 'porel</w>': 102, 'the</w>': 103, 'ele</w>': 104, 'sun</w>': 105, 'lon': 106, 'el': 107, 'ori': 108, 'ales</w>': 109, 'poil': 110, 'atime</w>': 111, 'por': 112, 'eti</w>': 113, 'erestin': 114, 'non</w>': 115, 'enois</w>': 116, '4': 117, 'renous</w>': 118, 'anon</w>': 119, 'al': 120, 'anout</w>': 121, 'areti': 122, 'riles</w>': 123, 'loring</w>': 124, 'relut</w>': 125, 'elim': 126, 'reth': 127, 'lou': 128, 'ering</w>': 129, 'asth': 130, 'erecom': 131, 'diles</w>': 132, 'inor</w>': 133, '*': 134, 'wit</w>': 135, 'i': 136, 'nou</w>': 137, 'onoring</w>': 138, 'esil': 139, 'roin': 140, 'alim': 141, 'rorem': 142, 'renom': 143, 'F': 144, 'eresting</w>': 145, 'asis</w>': 146, 'ily</w>': 147, 'ome</w>': 148, 'entinois</w>': 149, 'esim': 150, 'asu': 151, 'rome</w>': 152, 'le</w>': 153, 'em': 154, 'aling</w>': 155, 'apor': 156, 'iline</w>': 157, 'orely</w>': 158, 'aline</w>': 159, 'pou': 160, 'rilo': 161, 'dilo': 162, 'linon': 163, 'resting</w>': 164, 'estin': 165, 'an</w>': 166, 'rente</w>': 167, 'elo</w>': 168, 'onored</w>': 169, 'ding</w>': 170, 'tilin': 171, 'ous</w>': 172, 'relin': 173, 'oilu': 174, 'one</w>': 175, 'asin': 176, 'des</w>': 177, 'etin</w>': 178, 'of</w>': 179, 'elone</w>': 180, 'lour</w>': 181, 'sin': 182, 'elout</w>': 183, 'pon</w>': 184, 'ab': 185, 'dim': 186, 'siles</w>': 187, 'ast': 188, 'lout</w>': 189, 'red</w>': 190, 'erilous</w>': 191, 'arere': 192, 'was</w>': 193, 'ach': 194, 'asily</w>': 195, 'li</w>': 196, 'eune</w>': 197, 'asili': 198, 'resinous</w>': 199, 'res</w>': 200, 'aporer</w>': 201, 'noreri': 202, 'reren': 203, 'c': 204, 'te</w>': 205, 'R': 206, 'are': 207, 'ares': 208, 'ila</w>': 209, 'and</w>': 210, 'asil</w>': 211, 'aporeu': 212, 'alous</w>': 213, 'rily</w>': 214, 'pout</w>': 215, 'tile</w>': 216, 'inor': 217, 'ou': 218, 'ting</w>': 219, 'M': 220, 'pois</w>': 221, 'ilor': 222, 'orem': 223, 'alou</w>': 224, 'onore': 225, 'aril</w>': 226, 'til</w>': 227, 'oresi': 228, 'erethat</w>': 229, 'eth</w>': 230, 'dinous</w>': 231, 'asin</w>': 232, 'onorine</w>': 233, 'inois</w>': 234, 'sil': 235, 'qu': 236, 'rest</w>': 237, 'resu': 238, 'elo': 239, 'wim': 240, 'apor</w>': 241, 'erof</w>': 242, 'o': 243, 'resin</w>': 244, 'arec': 245, 'atilin': 246, 'aret': 247, 'S': 248, 'ant</w>': 249, 'h': 250, 'sut</w>': 251, 'onoi': 252, 'ti': 253, 'eth': 254, 'de</w>': 255, 'norin': 256, 'econ': 257, 'lorin</w>': 258, 'no</w>': 259, 'apore</w>': 260, 'erin</w>': 261, 'ron</w>': 262, 'alom': 263, 'con</w>': 264, 'et': 265, 'tiles</w>': 266, 'rine</w>': 267, 'st': 268, 'ronom': 269, 'oron': 270, 'ou</w>': 271, 'oreron': 272, 'u': 273, '-': 274, 'oi': 275, 'ereste</w>': 276, 'conome</w>': 277, 'onores</w>': 278, 'relo': 279, 'ing</w>': 280, 'esis</w>': 281, 'relis</w>': 282, 'si': 283, 'dit</w>': 284, 'B': 285, 'arine</w>': 286, 'orev': 287, 'il': 288, 'asino</w>': 289, 'erim': 290, 'lu': 291, 'ror': 292, 'etil</w>': 293, ',</w>': 294, 'ut</w>': 295, 'ese</w>': 296, 'orono</w>': 297, 'oris</w>': 298, 'erer': 299, 'el</w>': 300, 'eun</w>': 301, 'wing</w>': 302, 'dile</w>': 303, 'shis</w>': 304, 'recon': 305, 'eresu': 306, 'entine</w>': 307, 'erilou': 308, 'que</w>': 309, 'oreline</w>': 310, 'ilo': 311, 'erit</w>': 312, 'orese</w>': 313, 'ais</w>': 314, 'r': 315, 'ilou</w>': 316, 'wil': 317, 'reste</w>': 318, 'loi': 319, 'reli</w>': 320, 'reron': 321, 'anois</w>': 322, 'noi</w>': 323, 'pon': 324, 'elu': 325, 'ronou': 326, 'astime</w>': 327, 'lun</w>': 328, 'dino': 329, 'esile</w>': 330, 'arene</w>': 331, 'orerent</w>': 332, 'poiling</w>': 333, 'nore</w>': 334, 'rele</w>': 335, 'esu': 336, 'tino': 337, 'stime</w>': 338, 'ati</w>': 339, 'asing</w>': 340, 'reche</w>': 341, 'rero</w>': 342, 'asun</w>': 343, 'rento</w>': 344, 'lorentine</w>': 345, 'arer</w>': 346, 'eu': 347, 'tin': 348, 'stin': 349, 'ri': 350, 'ain</w>': 351, 'orim': 352, 'sti</w>': 353, 'nored</w>': 354, 'ere': 355, 'as': 356, 'pore': 357, 'rour</w>': 358, 'etil': 359, 'N': 360, 'eresh': 361, 'ono': 362, 'oreu': 363, 'rile</w>': 364, '6': 365, 'ld</w>': 366, 'rorer</w>': 367, 'ero': 368, 'din</w>': 369, 'alil': 370, 'lorine</w>': 371, 'ste</w>': 372, 'whe</w>': 373, 'ente</w>': 374, 'or': 375, 'oro</w>': 376, 'oron</w>': 377, 'arech': 378, 'ere</w>': 379, 'eloin': 380, 'sth': 381, 'she</w>': 382, 'rilit</w>': 383, 'eche</w>': 384, 'ailin': 385, 'aro</w>': 386, 'inone</w>': 387, 'ans</w>': 388, 'erentil': 389, 'renti</w>': 390, 'that</w>': 391, 'ate</w>': 392, 'nois</w>': 393, 'ronoun</w>': 394, 'oro': 395, 'enou</w>': 396, 'du': 397, 'linois</w>': 398, 'onori': 399, 'k': 400, 'oin': 401, 'esun</w>': 402, 'asto</w>': 403, '</u>': 404, '/': 405, 'pori': 406, 'at</w>': 407, 'stiles</w>': 408, 'ashe</w>': 409, 'sil</w>': 410, 'dut</w>': 411, 'oresti': 412, 'ilis</w>': 413, 'aune</w>': 414, 'erome</w>': 415, 'erest</w>': 416, 'lili': 417, 'alile</w>': 418, 'inon</w>': 419, 'erere</w>': 420, 'ilout</w>': 421, 'ad</w>': 422, 'relit</w>': 423, 'ore</w>': 424, 'entile</w>': 425, 'ce</w>': 426, 'an': 427, 'apo': 428, 'enoi': 429, 'ento</w>': 430, 'di</w>': 431, 'pou</w>': 432, 'stily</w>': 433, 'ace</w>': 434, 'eloi': 435, 'econom': 436, 'dil': 437, 'roring</w>': 438, 'ilit</w>': 439, 'ach</w>': 440, 'roine</w>': 441, 'oring</w>': 442, 'lorer': 443, 'alut</w>': 444, 'arent': 445, 'eles</w>': 446, 'erou': 447, 'asile</w>': 448, 'econe</w>': 449, 'aporent</w>': 450, 'ech</w>': 451, 'arome</w>': 452, 'ino': 453, 'this</w>': 454, 'in': 455, 'z': 456, 'alone</w>': 457, 'arely</w>': 458, 'onorit</w>': 459, 'lores</w>': 460, 'siloin': 461, 'arent</w>': 462, 'V': 463, 'esting</w>': 464, 'W': 465, '7': 466, 'arente</w>': 467, 'lof</w>': 468, 'reun</w>': 469, 'orech': 470, 'poi': 471, 'renon': 472, 'anori': 473, 'inorin': 474, 'rore</w>': 475, 'ilon': 476, 'enore</w>': 477, 'pof</w>': 478, 'ereth': 479, 'onorer': 480, 'reles</w>': 481, 'con': 482, 'ret</w>': 483, 'asil': 484, 'este</w>': 485, '</w>': 486, 'orelo': 487, 'retin': 488, 'entili': 489, 'roiling</w>': 490, 'anor': 491, 'tis</w>': 492, 'ilous</w>': 493, 'erily</w>': 494, 'rino': 495, 'sinon</w>': 496, 'anor</w>': 497, 'poil</w>': 498, 'f': 499, 'loron': 500, 'erous</w>': 501, 'rili': 502, 'D': 503, 'ailing</w>': 504, 'apoi': 505, 'arone</w>': 506, 'I': 507, 'sile</w>': 508, 'rero': 509, 'por</w>': 510, 'entime</w>': 511, 'les</w>': 512, 'sine</w>': 513, 'eresi': 514, 'roin</w>': 515, 'orine</w>': 516, 'alori': 517, 'erely</w>': 518, 'l': 519, 'y': 520, 'arenti': 521, 'roris</w>': 522, 'win</w>': 523, 'ecome</w>': 524, 'aile</w>': 525, 'anour</w>': 526, 'arily</w>': 527, 'ainous</w>': 528, 'rono': 529, 'res': 530, 'orere': 531, 'rerent</w>': 532, 'arethe</w>': 533, 'oiles</w>': 534, 'oile</w>': 535, 'ailor</w>': 536, 'ilori</w>': 537, 'rem': 538, 'nome</w>': 539, 'chis</w>': 540, 'tiling</w>': 541, 'eld</w>': 542, 'alon': 543, 'eme</w>': 544, 'orone</w>': 545, 'sis</w>': 546, 'on': 547, 'enor': 548, 'rout</w>': 549, 'ap': 550, 'dino</w>': 551, 'w</w>': 552, 'ail</w>': 553, 'ch': 554, 'atinous</w>': 555, 'inorit</w>': 556, 'ilim': 557, 'alino': 558, 'pom': 559, 'ano': 560, 'arere</w>': 561, 'aing</w>': 562, 'anom': 563, 'resi': 564, 'ril': 565, 'aili': 566, 'elit</w>': 567, 'lori': 568, 'arenth': 569, 'inoi': 570, 'lois</w>': 571, 'ret': 572, 'onorerent</w>': 573, 'ino</w>': 574, 'noi': 575, 'ar': 576, 'achis</w>': 577, 'nor': 578, 'erese</w>': 579, 'ethat</w>': 580, 'enti</w>': 581, 'en</w>': 582, 'ache</w>': 583, 'noring</w>': 584, 'enth': 585, 'orest': 586, 'eret</w>': 587, 'areu': 588, 'renti': 589, 'none</w>': 590, 'la</w>': 591, 'oret': 592, 'aro': 593, 'dune</w>': 594, 'orin': 595, 'ri</w>': 596, 'li': 597, 'ren</w>': 598, 'rone</w>': 599, 'rete</w>': 600, 'norer': 601, 'ailes</w>': 602, 'O': 603, 'aroi': 604, 'ent</w>': 605, 'ai</w>': 606, 'roil</w>': 607, 're': 608, '\"': 609, 'A': 610, 'onorer</w>': 611, 'sinore</w>': 612, 'alou': 613, 'alun</w>': 614, '!</w>': 615, 'pone</w>': 616, 'eline</w>': 617, 'elil': 618, 'rest': 619, 'atili': 620, 'eron': 621, 'am': 622, 'ilon</w>': 623, 'di': 624, 'orent</w>': 625, 'sit</w>': 626, 'aris</w>': 627, 'acon</w>': 628, 'nore': 629, 'reti': 630, 'enon': 631, 'tilit</w>': 632, 'lores': 633, 'nout</w>': 634, 'onore</w>': 635, 'rinoi': 636, 'anoin': 637, 'rec': 638, 'non': 639, 'norino</w>': 640, 'aron': 641, 'su': 642, 'etim': 643, 'eril': 644, 'lou</w>': 645, 'eretil': 646, 'ath</w>': 647, 'ed</w>': 648, 'aring</w>': 649, 'no': 650, 'time</w>': 651, 'rored</w>': 652, 'relu': 653, 'alo</w>': 654, 'er': 655, 'onon</w>': 656, 'eroin': 657, 't': 658, 'poinon': 659, 'orel</w>': 660, 'onon': 661, 'al</w>': 662, '9': 663, 'r</w>': 664, 'eti': 665, 'enthat</w>': 666, 'rela</w>': 667, 'porer</w>': 668, 'conois</w>': 669, 'eno': 670, 'esil</w>': 671, 'aime</w>': 672, 'che</w>': 673, 'or</w>': 674, 'roil': 675, 'lune</w>': 676, 'onorere': 677, 'g': 678, 'iling</w>': 679, 'ela</w>': 680, 'lorentin': 681, 's': 682, 'dine</w>': 683, 'X': 684, 'loil</w>': 685, 'erel': 686, 'ilori': 687, 'poreu': 688, 'asthe</w>': 689, 'elin</w>': 690, 'lom': 691, 'th': 692, 'alo': 693, 'enor</w>': 694, 'im': 695, 'enous</w>': 696, 'dime</w>': 697, 'arono</w>': 698, 'ore': 699, 'apores</w>': 700, 'inou': 701, 'erone</w>': 702, 'pores</w>': 703, 'atile</w>': 704, 'estine</w>': 705, 'agh': 706, 'cone</w>': 707, 'p': 708, 'ethe</w>': 709, 'lin</w>': 710, 'J': 711, 'astile</w>': 712, 'ailoring</w>': 713, 'rer</w>': 714, 'oim': 715, 'd</w>': 716, 'in</w>': 717, 'loit</w>': 718, 'anou</w>': 719, 'eli</w>': 720, 'ereto</w>': 721, 'erom': 722, 'line</w>': 723, 'ase</w>': 724, 'ai': 725, 'erine</w>': 726, 'ror</w>': 727, 'g</w>': 728, 'ilom': 729, 'ece</w>': 730, 'aine</w>': 731, 'alor': 732, 'apon': 733, 'eroit</w>': 734, 'are</w>': 735, 'enon</w>': 736, 'arino</w>': 737, 'esi': 738, 'v': 739, 'loreron': 740, 'win': 741, 'un</w>': 742, 'reling</w>': 743, 'av': 744, '0': 745, 'loris</w>': 746, 'arel</w>': 747, 'alu': 748, 'et</w>': 749, 'b': 750, 'arois</w>': 751, 'is</w>': 752, 'onou': 753, 'asiles</w>': 754, 'orer</w>': 755, 'esi</w>': 756, 'resti': 757, 'eror': 758, 'sime</w>': 759, 'L': 760, 'at': 761, '2': 762, 'lored</w>': 763, 'esino</w>': 764, 'resi</w>': 765, 'est</w>': 766, 'ro</w>': 767, 'eris</w>': 768, 'Q': 769, 'rorou</w>': 770, 'rononon': 771, 'dinou</w>': 772, 'd': 773, 'oil</w>': 774, 'je</w>': 775, 'esthat</w>': 776, 'm': 777, 'eli': 778, 'aron</w>': 779, 'sino</w>': 780, 'entil</w>': 781, 'elino</w>': 782, 'astim': 783, 'inous</w>': 784, 'on</w>': 785, 'sin</w>': 786, 'erene</w>': 787, 'esh': 788, 'rime</w>': 789, 'wine</w>': 790, 'enom': 791, 'anoi': 792, 'eroine</w>': 793, 'orecon': 794, 'esti</w>': 795, 'ne</w>': 796, 'atin': 797, 'lorentin</w>': 798, 'ores</w>': 799, 'alin</w>': 800, 'e</w>': 801, 'ono</w>': 802, 'lous</w>': 803, 'ar</w>': 804, 'ech': 805, 'orit</w>': 806, 'a': 807, 'ati': 808, 'eret': 809, 'dili': 810, 'sti': 811, 'erenti': 812, 'rent': 813, 'eut</w>': 814, 'aly</w>': 815, 'oren</w>': 816, 'ghis</w>': 817, 'lime</w>': 818, 'aim': 819, 'nores': 820, 'rou</w>': 821, 'oret</w>': 822, 'sinous</w>': 823, '`': 824, 'iloris</w>': 825, 'stil': 826, 'estil': 827, 'enoil': 828, 'stim': 829, 'inou</w>': 830, 'ly</w>': 831, 'oilune</w>': 832, 'aporis</w>': 833, 's</w>': 834, 'ile</w>': 835, 'orel': 836, 'lis</w>': 837, 'tili': 838, 'oreri': 839, 'wh': 840, 'roit</w>': 841, 'oily</w>': 842, 'asti': 843, 'C': 844, 'our</w>': 845, 'dil</w>': 846, 'rely</w>': 847, 'ala</w>': 848, 'areth</w>': 849, 'enou': 850, 'erecon': 851, 'dilu': 852, 'renonon': 853, 'com': 854, 'alon</w>': 855, 'onour</w>': 856, '.</w>': 857, 'norer</w>': 858, 'ereti': 859, 'areri': 860, 'po': 861, 'lori</w>': 862, 'esth': 863, 'enth</w>': 864, 'oren': 865, 'aresi': 866, 'recom': 867, 'ereu': 868, 'esit</w>': 869, 'tim': 870, \"'\": 871, 'erec': 872, 'alor</w>': 873, 'eror</w>': 874, 'astili': 875, 'erem': 876, 'lino</w>': 877, 'etin': 878, 'tinous</w>': 879, 'tin</w>': 880, 'arete</w>': 881, 'P': 882, 'erech': 883, 'roi</w>': 884, 'eno</w>': 885, 'rev': 886, 'areme</w>': 887, 'arest</w>': 888, 'orily</w>': 889, 'lor</w>': 890, 'ale</w>': 891, 'ailor': 892, 'ac': 893, 'ali</w>': 894, 'ren': 895, 'onorous</w>': 896, 'esthe</w>': 897, 'rese</w>': 898, 'l</w>': 899, 'norent</w>': 900, 'alila</w>': 901, 'silis</w>': 902, 'astil': 903, 'his</w>': 904, 'rece</w>': 905, 'elois</w>': 906, 'rere</w>': 907, 'arous</w>': 908, 'oin</w>': 909, 'ely</w>': 910, 'rit</w>': 911, 'wis</w>': 912, 'ared</w>': 913, 'elou': 914, 'onor</w>': 915, 'ail': 916, 'ec': 917, 'tilu': 918, 'ring</w>': 919, 'ril</w>': 920, 'it</w>': 921, 'arerent': 922, 'esto</w>': 923, '1': 924, 'asi</w>': 925, 'arit</w>': 926, 'rino</w>': 927, 'ild</w>': 928, 'stil</w>': 929, 'erel</w>': 930, 'oili': 931, 'wild</w>': 932, 'lil': 933, 'K': 934, 'alit</w>': 935, 'lorent</w>': 936, 'ev': 937, 'ilin': 938, '?': 939, 'eling</w>': 940, 'poring</w>': 941, 'arese</w>': 942, 'iles</w>': 943, 'w': 944, 'roi': 945, 'T': 946, 'o</w>': 947, 'sh': 948, 'q': 949, 'U': 950, 'ilin</w>': 951, 'pome</w>': 952, 'renou': 953, 'asi': 954, 'pous</w>': 955, 'onor': 956, 'erili': 957, 'a</w>': 958, 'erent': 959, 'lore': 960, 'ores': 961, 'silu': 962, 'es</w>': 963, 'ath': 964, 'stis</w>': 965, 'rentis</w>': 966, 'loin</w>': 967, 'ronome</w>': 968, 't</w>': 969, 'rom': 970, 'tilis</w>': 971, 'G': 972, 'reto</w>': 973, 'resh': 974, 'ene</w>': 975, 'oroi': 976, 'resin': 977, 'er</w>': 978, 'sim': 979, 'rere': 980, 'to</w>': 981, 'entin': 982, 'atis</w>': 983, 'nou': 984, 'asine</w>': 985, 'ored</w>': 986, 'eril</w>': 987, 'oiling</w>': 988, 'rene</w>': 989, 'noin': 990, 'y</w>': 991, 'pour</w>': 992, 'esti': 993, 'stili': 994, 'es': 995, 'ered</w>': 996, 'me</w>': 997, 'lim': 998, 'orou': 999, 'ete</w>': 1000, 'aret</w>': 1001, 'tine</w>': 1002, 'stine</w>': 1003, 'eshe</w>': 1004, 'onof</w>': 1005, 'erino</w>': 1006, 'tilo': 1007, 'rin</w>': 1008, 'oil': 1009, 'oreth': 1010, 'porel': 1011, 'stino</w>': 1012, 'estime</w>': 1013, 'elof</w>': 1014, 'pore</w>': 1015, 'lino': 1016, 'eresim': 1017, 'lo': 1018, 'inori': 1019, 'oi</w>': 1020, 'rou': 1021, 'reut</w>': 1022, 'ereth</w>': 1023, 'ilu': 1024, 'rel</w>': 1025, 'onorent</w>': 1026, 'eres</w>': 1027, 'alorou': 1028, 'nores</w>': 1029, 'st</w>': 1030, 'oreri</w>': 1031, 'alin': 1032, 'din': 1033, 'reune</w>': 1034, 'eri</w>': 1035, 'orin</w>': 1036, 'ron': 1037, 'erin': 1038, 'orethe</w>': 1039, 'wili': 1040, 'inon': 1041, 'lon</w>': 1042, 'aun</w>': 1043, 'atine</w>': 1044, '3': 1045, 'noun</w>': 1046, 'eri': 1047, 'elin': 1048, 'erest': 1049, 'eres': 1050, 'entin</w>': 1051, 'loi</w>': 1052, 'est': 1053, 'H': 1054, 'une</w>': 1055, 'aste</w>': 1056, 'reting</w>': 1057, 'aroi</w>': 1058, 'reno': 1059, 'astis</w>': 1060, 'nori': 1061, 'porous</w>': 1062, 'ash': 1063, 'reri': 1064, 'asthat</w>': 1065, 'out</w>': 1066, 'onous</w>': 1067, 'oril': 1068, 'ori</w>': 1069, 'ain': 1070, 'entil': 1071, 'gh': 1072, 'elon</w>': 1073, 'anou': 1074, 'entiles</w>': 1075, 'orecom': 1076, 'orer': 1077, 'elor': 1078, 'dis</w>': 1079, 'rous</w>': 1080, 'aily</w>': 1081, 'stile</w>': 1082, 'ch</w>': 1083, 'acom': 1084, 'ag</w>': 1085, 'arech</w>': 1086, 'om': 1087, 'erile</w>': 1088, '5': 1089, 'elune</w>': 1090, 'nono': 1091, 'Z': 1092, 'esin': 1093, 'anof</w>': 1094, 'n</w>': 1095, 'pored</w>': 1096, 'alis</w>': 1097, 'ime</w>': 1098, 'rois</w>': 1099, 'ali': 1100, 'ethis</w>': 1101, 'estis</w>': 1102, 'apon</w>': 1103, 'conom': 1104, 'norim': 1105, 'se</w>': 1106, 'stin</w>': 1107, 'nom': 1108, 'ari</w>': 1109, 'th</w>': 1110, 'atit</w>': 1111, 'asim': 1112, 'sthe</w>': 1113, 'aut</w>': 1114, 'ares</w>': 1115, 'en': 1116, 'conoit</w>': 1117, 'acon': 1118, 'ato</w>': 1119, 'lorous</w>': 1120, 'loren': 1121, 'ane</w>': 1122, 'asting</w>': 1123, 'enting</w>': 1124, 'ine</w>': 1125, 'onom': 1126, 'x': 1127, 'relim': 1128, 'sting</w>': 1129, 'reli': 1130, 'atin</w>': 1131, 'oila</w>': 1132, 'ling</w>': 1133, 'eroi': 1134, 'reme</w>': 1135, 'entim': 1136, 'ag': 1137, 'elis</w>': 1138, 'rim': 1139, 'sune</w>': 1140, 'apour</w>': 1141, 'lo</w>': 1142, 'ait</w>': 1143, 'ano</w>': 1144, 'arel': 1145, 'lit</w>': 1146, 'aren': 1147, 'si</w>': 1148, 'reres</w>': 1149, 'arethat</w>': 1150, 'oit</w>': 1151, 'as</w>': 1152, 'sthat</w>': 1153, 'come</w>': 1154, 'erent</w>': 1155, 'erer</w>': 1156, 'onorou': 1157, 'tino</w>': 1158, 'qut</w>': 1159, 'orous</w>': 1160, 'lilo': 1161, 'oing</w>': 1162, 'j': 1163, 'oilin': 1164, 'athe</w>': 1165, 'ilo</w>': 1166, 'astin': 1167, '[]': 1168, 'anoine</w>': 1169, 'tily</w>': 1170, 'rent</w>': 1171, 'entit</w>': 1172, 'oreune</w>': 1173, 'dun</w>': 1174, 'esine</w>': 1175, 'etime</w>': 1176, 'arem': 1177, 'estin</w>': 1178, 'areno': 1179, 'orest</w>': 1180, 'rin': 1181, 'lore</w>': 1182, 'ris</w>': 1183, 'alour</w>': 1184, 'rel': 1185, 'restin': 1186, 'ailu': 1187, 'atiline</w>': 1188, 'ent': 1189, 'arin</w>': 1190, 'esing</w>': 1191, 'ereno': 1192, 'alois</w>': 1193, 'nous</w>': 1194, 'lily</w>': 1195, 'athis</w>': 1196, 'ame</w>': 1197, 'il</w>': 1198, 'ro': 1199, 'I</w>': 1200, 'oine</w>': 1201, 'anous</w>': 1202, 'tit</w>': 1203, 'erono': 1204, 'he</w>': 1205, 'e': 1206}\n",
      "vocab size: 1207\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import tqdm as tqdm\n",
    "import pandas as pd\n",
    "dataset = datasets.load_dataset(\"opus_books\", \"en-fr\")\n",
    "en_content = ''.join([dataset[\"train\"][i][\"translation\"][\"en\"] for i in range(dataset.num_rows[\"train\"])])\n",
    "fr_content = ''.join([dataset[\"train\"][i][\"translation\"][\"fr\"] for i in range(dataset.num_rows[\"train\"])])\n",
    "global_content = en_content + fr_content\n",
    "\n",
    "tokenizer = BPE_Tokenizer()\n",
    "\n",
    "num_merges =  90\n",
    "untils = []\n",
    "total_tokens = []\n",
    "avg_token_len = []\n",
    "avg_token_std = []\n",
    "\n",
    "def clean(text: str) -> str:\n",
    "    text = text.replace('.', ' . ')\n",
    "    text = text.replace(',', ' , ')\n",
    "    text = text.replace('!', ' ! ')\n",
    "    text = text.replace('?', ' ? ')\n",
    "    text = text.replace(':', ' : ')\n",
    "    text = text.replace(';', ' ; ')\n",
    "    text = text.replace(')', '')\n",
    "    text = text.replace('(', '')\n",
    "    text = text.replace('@', '')\n",
    "    text = text.replace('|', '')\n",
    "    text = text.replace(']', '')\n",
    "    text = text.replace('[', '')\n",
    "    text = text.replace('~', '')\n",
    "    text = text.replace('^', '')\n",
    "    text = text.replace('<', '')\n",
    "    text = text.replace('>', '')\n",
    "    text = text.replace('&', '')\n",
    "    text = text.replace('{', '')\n",
    "    text = text.replace('}', '')\n",
    "    text = text.replace('+', '')\n",
    "    # text = text.replace('-', '')\n",
    "    text = text.replace('tititi', '')\n",
    "    text = text.replace('orerer', '')\n",
    "    text = text.replace('errero', '')\n",
    "    text = text.replace('\\u007f', '')\n",
    "    text = text.replace('_', '')\n",
    "    text = text.replace('%', '')\n",
    "    text = text.replace('$', '')\n",
    "    text = text.replace('\\\\', '')\n",
    "    text = text.replace('=', '')\n",
    "    text = text.replace('#', '')\n",
    "    text = text.replace(';', '')\n",
    "    text = text.replace(':', '')\n",
    "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
    "    return text\n",
    "\n",
    "\n",
    "global_content = clean(global_content)\n",
    "print(\"text length:\", len(global_content))\n",
    "print(\"Training...\")\n",
    "tokenizer.train_until(global_content, 1200)\n",
    "# tokenizer.load_vocab('token_to_index.json')\n",
    "\n",
    "print(\"Vocab:\", tokenizer.token_to_index)\n",
    "print(\"vocab size:\", len(tokenizer))\n",
    "\n",
    "\n",
    "tokenizer.add_special_tokens(['<PAD></w>', '<SOS></w>','<EOS></w>'])\n",
    "\n",
    "\n",
    "# Saving token_to_index mapping\n",
    "tokenizer.save_vocab('en-fr.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [1130, 455, 713, 803, 644, 754, 1166, 198, 38]\n",
      "num Tokens: 9\n",
      "Detokenized Text: <PAD> <SOS> Je suis jeune <EOS> \n"
     ]
    }
   ],
   "source": [
    "# Tokenizing\n",
    "token_ids = tokenizer.tokenize(\"<PAD> <SOS> Je suis jeune <EOS>\")\n",
    "print(\"Token IDs:\", token_ids)\n",
    "print(\"num Tokens:\", len(token_ids))\n",
    "\n",
    "# Detokenizing\n",
    "detokenized_text = tokenizer.detokenize(token_ids)\n",
    "print(\"Detokenized Text:\", detokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[455]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"<SOS>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilingualDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer: BPE_Tokenizer, seq_len) -> None:\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.sos_idx = torch.tensor(self.tokenizer.tokenize(\"<SOS>\"), dtype = torch.int64)\n",
    "        self.eos_idx = torch.tensor(self.tokenizer.tokenize(\"<EOS>\"), dtype = torch.int64)\n",
    "        self.pad_idx = torch.tensor(self.tokenizer.tokenize(\"<PAD>\"), dtype = torch.int64)\n",
    "        \n",
    "    def _causal_mask(self, seq_len: int) -> torch.Tensor:\n",
    "        mask = torch.ones(1, seq_len, seq_len, dtype=torch.bool)\n",
    "        mask = torch.tril(mask, diagonal=0)\n",
    "        return mask\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        src = torch.tensor(self.tokenizer.tokenize(clean(self.dataset[idx][\"en\"])), dtype = torch.int64)\n",
    "        tgt = torch.tensor(self.tokenizer.tokenize(clean(self.dataset[idx][\"fr\"])), dtype = torch.int64)\n",
    "        if self.seq_len - len(src) - 2 < 0 or self.seq_len - len(tgt) - 1 < 0:\n",
    "            src = src[:self.seq_len - 2]\n",
    "            tgt = tgt[:self.seq_len - 1]\n",
    "        enc_num_pad = self.seq_len - len(src) - 2\n",
    "        dec_num_pad = self.seq_len - len(tgt) - 1\n",
    "        input_src = torch.cat([self.sos_idx, src, self.eos_idx, self.pad_idx.repeat(enc_num_pad)])\n",
    "        input_tgt = torch.cat([self.sos_idx, tgt, self.pad_idx.repeat(dec_num_pad)])\n",
    "        input_label = torch.cat([tgt,self.eos_idx, self.pad_idx.repeat(dec_num_pad)])\n",
    "        return (\n",
    "            input_src, \n",
    "            input_tgt, \n",
    "            input_label, \n",
    "            (input_src!=self.pad_idx).unsqueeze(0).unsqueeze(0).int() == 1,\n",
    "            (input_tgt!=self.pad_idx).unsqueeze(0).unsqueeze(0).int() & self._causal_mask(self.seq_len) == 1,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [x for x in dataset[\"train\"][\"translation\"] if len(tokenizer.tokenize(x[\"en\"])) < 100 and len(tokenizer.tokenize(x[\"fr\"]))  < 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = dataset[:int(len(dataset)*0.8)]\n",
    "val_set = dataset[int(len(dataset)*0.8):int(len(dataset)*0.9)]\n",
    "test_set = dataset[int(len(dataset)*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_seq_len = 0\n",
    "# sum = 0\n",
    "# for i in range(len(train_set)):\n",
    "#     src = train_set[i][\"en\"]\n",
    "#     tgt = train_set[i][\"fr\"]\n",
    "#     max_seq_len = max(max_seq_len, len(tokenizer.tokenize(src)), len(tokenizer.tokenize(tgt)))\n",
    "#     sum+=len(tokenizer.tokenize(src))\n",
    "# mean = sum/len(train_set)\n",
    "# max_seq_len, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = BilingualDataset(train_set, tokenizer, 100)\n",
    "val_set = BilingualDataset(val_set, tokenizer, 100)\n",
    "test_set = BilingualDataset(test_set, tokenizer, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83537, 10442, 10443)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_train), len(val_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_src, input_tgt, input_label, src_mask, tgt_mask = dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100]),\n",
       " torch.Size([100]),\n",
       " torch.Size([100]),\n",
       " torch.Size([1, 1, 100]),\n",
       " torch.Size([1, 100, 100]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tgt.shape, input_src.shape, input_label.shape, src_mask.shape, tgt_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_head=6,\n",
    "    embed_size=600,\n",
    "    context_length=100,\n",
    "    dropout=0.1,\n",
    "    num_layers=6,\n",
    "    device=device,\n",
    ")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "lr = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> \"The smile is very well , \" said he , catching instantly the passing expression \"but speak too . \" <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "8porelenteloin`erestabronoRtilit aly renouare lily ino 2elon lorowine rilatharely eloinrenononering eroiapour `sis sis astis sinore wily entinacerile orrour onorietimrenouastily renourenoustily renourenourenouwte renourenourenourenourenourenourenouagrenourenourenourenourenourenourenourenourenourenourenourenoureli renourenouagagrenourenouagrenourenourenouapore apore tilorenouwrenouagrenourenourenourenourenourenourenourenourenourenoustily \n",
      "-- Voil un sourire qui me plat , dit-il , mais cela ne suffit pas parlez . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss 5.258 | Acc 0.535: : 2it [00:00,  2.99it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss 1.163 | Acc 0.729: : 1001it [04:30,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> I drank for forgetfulness , and when I woke next day I was beside the count . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "JJe nis , mvrer , et je and je ne paperille ais , ceucant ain , je 'tais p'la c.  mme . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "\"Je bus pour oublier , et quand je me rveillai le lendemain , j'tais dans le lit du comte . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss 1.434 | Acc 0.667: : 2001it [09:04,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> All these crumbling masses were covered with an enamel polished by the action of underground fires , and they glistened under the stream of electric light from our beacon . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "Ehtes ces coses , 'eres abl, et urertes , 'Acode sde oules 'accon de cur, ouvaivs es , et secemda aiaient , tctrcde cus de 'tueues de  camc. <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "Toutes ces masses dsagrges , recouvertes d'un mail poli sous l'action des feux souterrains , resplendissaient au contact des jets lectriques du fanal . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss 1.015 | Acc 0.753: : 2610it [11:51,  3.67it/s]\n",
      "Epoch 0 | Loss 1.093: 100%|██████████| 326/326 [00:29<00:00, 11.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1Validation Loss: 1.060110277564241\n",
      "Validation Loss Decreased(inf--->11069.671518) \t Saving The Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss 1.018 | Acc 0.746: : 1it [00:00,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> Pain is always by the side of joy , the spondee by the dactyl . Master , I must relate to you the history of the Barbeau mansion . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "Je memleur de la out jours la aumla Mue , je Pod ,  rs de  Pocre s de <EOS>  est ari re , je est aut que je vous ai sla ette mistoire .  mce . eny on y  . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "La douleur est toujours ct de la joie , le sponde auprs du dactyle . Mon matre , il faut que je vous conte cette histoire du logis Barbeau . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss 0.970 | Acc 0.751: : 1001it [04:34,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> Cyrus Harding and his companions could not understand it . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "Cyrus Smith et ne es compagnons ne pardrent ent . es compagdre . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "Cyrus Smith et ses compagnons regardaient sans comprendre . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss 0.782 | Acc 0.805: : 2001it [09:08,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> \"My friends , \" I said , \"we're in a serious predicament , but I'm counting on your courage and energy . \" <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "-a amis , dit s-je , mmtuation , prand e , et on je suprendr votre amourage , vr motre amegie . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "Mes amis , dis-je , la situation est grave , mais je compte sur votre courage et sur votre nergie . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss 1.051 | Acc 0.736: : 2610it [11:54,  3.65it/s]\n",
      "Epoch 1 | Loss 0.925: 100%|██████████| 326/326 [00:28<00:00, 11.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2Validation Loss: 0.9462678233820505\n",
      "Validation Loss Decreased(11069.671518--->9880.928612) \t Saving The Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Loss 0.806 | Acc 0.795: : 1it [00:00,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> They have always been together , and according to his account he has been a very lonely man with only her as a companion , so that the thought of losing her was really terrible to him . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "Cl s ne t poujours peru qucemble , stporehxcstence de ans eude aire , elque t'ltrzer , clusonection e , sclusre , pvait ponn psi t'bler . out rible . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "Ils ont toujours vcu ensemble et il a men une existence solitaire quelle seule gayait la perspective de la perdre ne pouvait donc que lui sembler terrible . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Loss 1.089 | Acc 0.725: : 1001it [04:34,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> It is true , that while I worked , she would idle and I thought to myself , \"If you and I were destined to live always together , cousin , we would commence matters on a different footing . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "Se vais s tonc vbsge , vipoter ,  si , ien que je ssible , metiaites , je mves s ons , vethprit , aire le , et je vertmicomaleux . vourre . que porraer . 'comrettes . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "Je fus donc oblige de supporter aussi bien que possible les plaintes et les lamentations de cet esprit faible , et je fis de mon mieux pour coudre et emballer ses toilettes . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Loss 0.891 | Acc 0.776: : 2001it [09:08,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> He was making his blood too thick by going to sleep every evening after dinner . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "Il n'arsait ssait sdoig de ous fre dinir , erque joir , rs sdner . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "Il spaississait le sang sendormir chaque soir aprs le dner . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Loss 0.863 | Acc 0.780: : 2610it [11:54,  3.65it/s]\n",
      "Epoch 2 | Loss 1.011: 100%|██████████| 326/326 [00:28<00:00, 11.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3Validation Loss: 0.9017884768281625\n",
      "Validation Loss Decreased(9880.928612--->9416.475275) \t Saving The Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Loss 0.954 | Acc 0.752: : 1it [00:00,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> The minutes passed very slowly fifteen were counted before the library- door again opened . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "Les cemps tasut dgde inart de 'homre , 'inril, ous c'il tendait , vrir , porte . la voubliothque . cin . et r le . ngoal  . int . ar la voule . ogre . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "Le temps parut long un quart d'heure s'coula sans qu'on entendt ouvrir la porte de la bibliothque enfin , Mlle Ingram revint par la salle manger . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Loss 0.773 | Acc 0.798: : 1001it [04:34,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> \"Valuable ? \" returned Pencroft . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "-- Eouuisx ? rpondit Pencroff . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "-- Prcieuse ? rpondit Pencroff . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Loss 0.891 | Acc 0.775: : 2001it [09:08,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> Our wagonette had topped a rise and in front of us rose the huge expanse of the moor , mottled with gnarled and craggy cairns and tors . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "Notre poiture avait pteint la caut de lmo, ans ant nous , ait dre tporetde , et ltsde cled es et tves et de cois tagenous ame . pess les . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "Notre voiture avait atteint le haut de la cte devant nous stendait la lande , parseme de pics coniques et de monts-joie en dentelles . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Loss 0.830 | Acc 0.796: : 2610it [11:55,  3.65it/s]\n",
      "Epoch 3 | Loss 0.773: 100%|██████████| 326/326 [00:28<00:00, 11.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4Validation Loss: 0.8718183086740858\n",
      "Validation Loss Decreased(9416.475275--->9103.526779) \t Saving The Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Loss 0.716 | Acc 0.816: : 1it [00:00,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> M de Treville approved of the resolution he had adopted , and assured him that if on the morrow he did not appear , he himself would undertake to find him , let him be where he might . <EOS> <PAD> <PAD> <PAD> \n",
      "L . de Trville , prova qutuolution , il il avait pris se , et il 'insra que , il il vendemain , av'avait pas enprou , il se 'tait sien slurouver . il i , il arut out . il averait . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "M . de Trville approuva la rsolution qu'il avait prise , et l'assura que , si le lendemain il n'avait pas reparu , il saurait bien le retrouver , lui , partout o il serait . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Loss 0.784 | Acc 0.799: : 1001it [04:34,  3.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> Then he rose , and paced slowly up and down the room , his chin sunk upon his breast . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "Ae sque il il se teva , et se lit en rienter su'ement et miece et fas ssa sumte . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "Lorsqu'il se releva , il se mit arpenter lentement la pice en baissant la tte . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Loss 0.884 | Acc 0.771: : 2001it [09:08,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> Quick , now ! ' <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "Allons , cocheus -nous ! <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "Allons , dpchons-nous . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Loss 0.724 | Acc 0.810: : 2610it [11:55,  3.65it/s]\n",
      "Epoch 4 | Loss 0.759: 100%|██████████| 326/326 [00:28<00:00, 11.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5Validation Loss: 0.8573653238117867\n",
      "Validation Loss Decreased(9103.526779--->8952.608711) \t Saving The Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Loss 0.714 | Acc 0.818: : 1it [00:00,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> Next day , the Marquis took Julien to a lonely mansion , at some distance from Paris . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "Pe lendemain , Jmarquis , vit sit Julien , ineeau de col . sez longn . Paris . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "Le lendemain , le marquis conduisit Julien un chteau isol assez loign de Paris . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Loss 0.735 | Acc 0.808: : 1001it [04:34,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> It is a perpetual \"I love you . \" <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "-est une pvous aime . ersuuel . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "Cest un je vous aime perptuel . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Loss 0.912 | Acc 0.762: : 2001it [09:08,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> \"What then , Die ? \" he replied , maintaining a marble immobility of feature . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "-- Qt  bien , quareua ! il prit t-il , se tervant mmimporobile de marbre . il x bien ! <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "-- Eh bien ! Diana , reprit-il en conservant la mme immobilit de marbre , eh bien ! <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Loss 0.729 | Acc 0.812: : 2322it [10:36,  3.65it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(output\u001b[38;5;241m.\u001b[39mview(B \u001b[38;5;241m*\u001b[39m T, C), label\u001b[38;5;241m.\u001b[39mview(B \u001b[38;5;241m*\u001b[39m T), ignore_index\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mtokenize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[PAD]\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     18\u001b[0m acc \u001b[38;5;241m=\u001b[39m (output\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m label)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m---> 19\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     21\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/createch/IA/A5/wite_me_a_poeme/.venv/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/createch/IA/A5/wite_me_a_poeme/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "min_valid_loss = np.inf\n",
    "for _ in range(num_epochs):\n",
    "    model.train()\n",
    "    with tqdm.tqdm(enumerate(train_loader)) as pbar:\n",
    "        for idx, (src, tgt, label, src_mask, tgt_mask) in pbar:\n",
    "            src = src.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "            label = label.to(device)\n",
    "            src_mask = src_mask.to(device)\n",
    "            tgt_mask = tgt_mask.to(device)\n",
    "            output = model(src, tgt, src_mask, tgt_mask)\n",
    "            B, T, C = output.shape\n",
    "            if idx%1000 == 0:\n",
    "                print(tokenizer.detokenize(src[0].tolist()))\n",
    "                print(tokenizer.detokenize(output.argmax(dim=-1)[0].tolist()))\n",
    "                print(tokenizer.detokenize(label[0].tolist()))\n",
    "            loss = F.cross_entropy(output.view(B * T, C), label.view(B * T), ignore_index=tokenizer.tokenize(\"[PAD]\")[0])\n",
    "            acc = (output.argmax(dim=-1) == label).float().mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            pbar.set_description(f\"Epoch {_} | Loss {loss.item():.3f} | Acc {acc.item():.3f}\")\n",
    "            # break\n",
    "    valid_loss = 0\n",
    "    model.eval()\n",
    "    with tqdm.tqdm(val_loader) as pbar:\n",
    "        with torch.no_grad():\n",
    "            for src, tgt, label, src_mask, tgt_mask in pbar:\n",
    "                src = src.to(device)\n",
    "                tgt = tgt.to(device)\n",
    "                label = label.to(device)\n",
    "                src_mask = src_mask.to(device)\n",
    "                tgt_mask = tgt_mask.to(device)\n",
    "                \n",
    "                output = model(src, tgt, src_mask, tgt_mask)\n",
    "                loss = F.cross_entropy(output.view(B * T, C), label.view(B * T), ignore_index=tokenizer.tokenize(\"[PAD]\")[0])\n",
    "                valid_loss += loss.item()*src.shape[0]\n",
    "                pbar.set_description(f\"Epoch {_} | Loss {loss.item():.3f}\")\n",
    "    print(f'Epoch {_+1}Validation Loss: {valid_loss / len(val_set)}')\n",
    "    if min_valid_loss > valid_loss:\n",
    "        print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n",
    "        min_valid_loss = valid_loss\n",
    "        # Saving State Dict\n",
    "        torch.save(model.state_dict(), 'saved_model_tokenizer_3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (embed): Embedding(1210, 600)\n",
       "    (pos_encoding): SinusoidEncoding()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (qkv): Linear(in_features=600, out_features=1800, bias=False)\n",
       "        (mha): MultiHeadAttention(\n",
       "          (fc_out): Linear(in_features=600, out_features=600, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (fc_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ffwd): FeedForward(\n",
       "          (0): Linear(in_features=600, out_features=2400, bias=False)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=2400, out_features=600, bias=False)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm1): LayerNorm((600,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((600,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embed): Embedding(1210, 600)\n",
       "    (pos_encoding): SinusoidEncoding()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x DecoderBlock(\n",
       "        (qkv): Linear(in_features=600, out_features=1800, bias=False)\n",
       "        (mmha): MultiHeadAttention(\n",
       "          (fc_out): Linear(in_features=600, out_features=600, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (fc_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mha): MultiHeadAttention(\n",
       "          (fc_out): Linear(in_features=600, out_features=600, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (fc_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ffwd): FeedForward(\n",
       "          (0): Linear(in_features=600, out_features=2400, bias=False)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=2400, out_features=600, bias=False)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm1): LayerNorm((600,), eps=1e-05, elementwise_affine=True)\n",
       "        (normraw): LayerNorm((600,), eps=1e-05, elementwise_affine=True)\n",
       "        (normenc): LayerNorm((600,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((600,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=600, out_features=1210, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "model = Transformer(\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_head=6,\n",
    "    embed_size=600,\n",
    "    context_length=100,\n",
    "    dropout=0.1,\n",
    "    num_layers=6,\n",
    "    device=device,\n",
    ")\n",
    "model.load_state_dict(torch.load('saved_model_tokenizer_3.pth'))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([455]) tensor([38]) tensor([1130])\n",
      "torch.Size([1, 100]) torch.Size([1, 1]) torch.Size([1, 1, 100]) torch.Size([1, 1, 1, 1])\n",
      "torch.int64 torch.int64 torch.bool torch.bool\n",
      "torch.Size([1, 1, 1210])\n",
      "tensor([713])\n",
      "tensor([455, 713])\n",
      "torch.Size([1, 100]) torch.Size([1, 2]) torch.Size([1, 1, 100]) torch.Size([1, 1, 2, 2])\n",
      "torch.int64 torch.int64 torch.bool torch.bool\n",
      "torch.Size([1, 2, 1210])\n",
      "tensor([803])\n",
      "tensor([455, 713, 803])\n",
      "torch.Size([1, 100]) torch.Size([1, 3]) torch.Size([1, 1, 100]) torch.Size([1, 1, 3, 3])\n",
      "torch.int64 torch.int64 torch.bool torch.bool\n",
      "torch.Size([1, 3, 1210])\n",
      "tensor([644])\n",
      "tensor([455, 713, 803, 644])\n",
      "torch.Size([1, 100]) torch.Size([1, 4]) torch.Size([1, 1, 100]) torch.Size([1, 1, 4, 4])\n",
      "torch.int64 torch.int64 torch.bool torch.bool\n",
      "torch.Size([1, 4, 1210])\n",
      "tensor([754])\n",
      "tensor([455, 713, 803, 644, 754])\n",
      "torch.Size([1, 100]) torch.Size([1, 5]) torch.Size([1, 1, 100]) torch.Size([1, 1, 5, 5])\n",
      "torch.int64 torch.int64 torch.bool torch.bool\n",
      "torch.Size([1, 5, 1210])\n",
      "tensor([744])\n",
      "tensor([455, 713, 803, 644, 754, 744])\n",
      "torch.Size([1, 100]) torch.Size([1, 6]) torch.Size([1, 1, 100]) torch.Size([1, 1, 6, 6])\n",
      "torch.int64 torch.int64 torch.bool torch.bool\n",
      "torch.Size([1, 6, 1210])\n",
      "tensor([660])\n",
      "tensor([455, 713, 803, 644, 754, 744, 660])\n",
      "torch.Size([1, 100]) torch.Size([1, 7]) torch.Size([1, 1, 100]) torch.Size([1, 1, 7, 7])\n",
      "torch.int64 torch.int64 torch.bool torch.bool\n",
      "torch.Size([1, 7, 1210])\n",
      "tensor([54])\n",
      "tensor([455, 713, 803, 644, 754, 744, 660,  54])\n",
      "torch.Size([1, 100]) torch.Size([1, 8]) torch.Size([1, 1, 100]) torch.Size([1, 1, 8, 8])\n",
      "torch.int64 torch.int64 torch.bool torch.bool\n",
      "torch.Size([1, 8, 1210])\n",
      "tensor([38])\n",
      "tensor([455, 713, 803, 644, 754, 744, 660,  54,  38])\n",
      "<SOS> Je suis un tre <EOS> \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def gen(model: nn.Module, sentence: str, max_len: int, vocab: BPE_Tokenizer, device: torch.device):\n",
    "    model.eval()\n",
    "    sos_token = torch.tensor(vocab.tokenize(\"<SOS>\"), dtype=torch.int64).to(device)\n",
    "    eos_token = torch.tensor(vocab.tokenize(\"<EOS>\"), dtype=torch.int64).to(device)\n",
    "    pad_token = torch.tensor(vocab.tokenize(\"<PAD>\"), dtype=torch.int64).to(device)\n",
    "    print(sos_token, eos_token, pad_token)\n",
    "    \n",
    "    src_input = torch.cat([sos_token, torch.tensor(vocab.tokenize(sentence), dtype=torch.int64).to(device), eos_token, pad_token.repeat(max_len - len(vocab.tokenize(sentence)) - 2)])\n",
    "    src_mask = (src_input != pad_token).unsqueeze(0).int() == 1\n",
    "    \n",
    "    tgt_input = sos_token\n",
    "    while tgt_input[-1] != eos_token and len(tgt_input) < max_len:\n",
    "        tgt_mask = dataset_train._causal_mask(tgt_input.shape[0]) == 1\n",
    "        src_input, tgt_input, src_mask, tgt_mask = src_input.to(device), tgt_input.to(device), src_mask.to(device), tgt_mask.to(device)\n",
    "        print(src_input.unsqueeze(0).shape, tgt_input.unsqueeze(0).shape, src_mask.unsqueeze(0).shape, tgt_mask.unsqueeze(0).shape)\n",
    "        print(src_input.unsqueeze(0).dtype, tgt_input.unsqueeze(0).dtype, src_mask.unsqueeze(0).dtype, tgt_mask.unsqueeze(0).dtype)\n",
    "\n",
    "        logits = model(src_input.unsqueeze(0), tgt_input.unsqueeze(0), src_mask.unsqueeze(0), tgt_mask.unsqueeze(0))\n",
    "        pred = F.softmax(logits, dim=-1)\n",
    "        print(pred.shape)\n",
    "        print(pred[:, -1, :].argmax(dim=-1))\n",
    "        # next_token = torch.multinomial(pred[:,-1,:], num_samples=1)\n",
    "        tgt_input = torch.cat([tgt_input, pred[:,-1,:].argmax(dim = -1).to(device)])\n",
    "        print(tgt_input)\n",
    "    print(vocab.detokenize(tgt_input.tolist()))\n",
    "gen(model, \"I am a student\", 100, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
