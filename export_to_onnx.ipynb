{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some standard imports\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "import torch.onnx\n",
    "from new_transformer import Transformer\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from onnxruntime.quantization import quantize_dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (embed): Embedding(1210, 600)\n",
       "    (pos_encoding): SinusoidEncoding()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (qkv): Linear(in_features=600, out_features=1800, bias=False)\n",
       "        (mha): MultiHeadAttention(\n",
       "          (fc_out): Linear(in_features=600, out_features=600, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (fc_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ffwd): FeedForward(\n",
       "          (0): Linear(in_features=600, out_features=2400, bias=False)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=2400, out_features=600, bias=False)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm1): LayerNorm((600,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((600,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embed): Embedding(1210, 600)\n",
       "    (pos_encoding): SinusoidEncoding()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x DecoderBlock(\n",
       "        (qkv): Linear(in_features=600, out_features=1800, bias=False)\n",
       "        (mmha): MultiHeadAttention(\n",
       "          (fc_out): Linear(in_features=600, out_features=600, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (fc_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mha): MultiHeadAttention(\n",
       "          (fc_out): Linear(in_features=600, out_features=600, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (fc_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ffwd): FeedForward(\n",
       "          (0): Linear(in_features=600, out_features=2400, bias=False)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=2400, out_features=600, bias=False)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm1): LayerNorm((600,), eps=1e-05, elementwise_affine=True)\n",
       "        (normraw): LayerNorm((600,), eps=1e-05, elementwise_affine=True)\n",
       "        (normenc): LayerNorm((600,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((600,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=600, out_features=1210, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = Transformer(\n",
    "    vocab_size=1210,\n",
    "    n_head=6,\n",
    "    embed_size=600,\n",
    "    context_length=100,\n",
    "    dropout=0.1,\n",
    "    num_layers=6,\n",
    "    device=device,\n",
    ")\n",
    "model.load_state_dict(torch.load(\"saved_model_tokenizer_3.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]]) tensor([[[[True, True, True, True, True, True, True, True, True, True, True,\n",
      "           True, True, True, True, True, True, True, True, True, True, True,\n",
      "           True, True, True, True, True, True, True, True, True, True, True,\n",
      "           True, True, True, True, True, True, True, True, True, True, True,\n",
      "           True, True, True, True, True, True, True, True, True, True, True,\n",
      "           True, True, True, True, True, True, True, True, True, True, True,\n",
      "           True, True, True, True, True, True, True, True, True, True, True,\n",
      "           True, True, True, True, True, True, True, True, True, True, True,\n",
      "           True, True, True, True, True, True, True, True, True, True, True,\n",
      "           True]]]]) tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0]]) tensor([[[[True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          ...,\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True],\n",
      "          [True, True, True,  ..., True, True, True]]]])\n",
      "torch.Size([1, 100]) torch.Size([1, 1, 1, 100]) torch.Size([1, 99]) torch.Size([1, 1, 99, 99])\n",
      "torch.int64 torch.bool torch.int64 torch.bool\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.0089, -2.4957, -7.8957,  ..., -7.9997, -3.8218,  2.9767],\n",
       "         [-4.0081, -2.4983, -7.8962,  ..., -7.9999, -3.8231,  2.9747],\n",
       "         [-4.0067, -2.5006, -7.8970,  ..., -8.0007, -3.8239,  2.9733],\n",
       "         ...,\n",
       "         [-4.0073, -2.5028, -7.8980,  ..., -8.0025, -3.8190,  2.9798],\n",
       "         [-4.0079, -2.5008, -7.8993,  ..., -8.0047, -3.8187,  2.9804],\n",
       "         [-4.0080, -2.4980, -7.8998,  ..., -8.0056, -3.8176,  2.9818]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 1\n",
    "src = torch.zeros((batch_size, 100),dtype= torch.int64)\n",
    "src_mask = torch.ones((batch_size, 1, 1, 100), dtype= torch.bool)\n",
    "tgt = torch.zeros((batch_size, 99),dtype= torch.int64)\n",
    "tgt_mask = torch.ones((batch_size, 1, 99, 99), dtype= torch.bool)\n",
    "print(src, src_mask, tgt, tgt_mask)\n",
    "\n",
    "print(src.shape, src_mask.shape, tgt.shape, tgt_mask.shape)\n",
    "print(src.dtype, src_mask.dtype, tgt.dtype, tgt_mask.dtype)\n",
    "\n",
    "torch_out = model(src, tgt, src_mask, tgt_mask)\n",
    "torch_out.shape\n",
    "torch_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(model,               # model being run\n",
    "                  (src,tgt,src_mask,tgt_mask),                         # model input (or a tuple for multiple inputs)\n",
    "                  \"model_1.onnx\",   # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=17,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['source', 'target', 'source_mask', 'target_mask'],   # the model's input names\n",
    "                  output_names = ['output'], # the model's output names\n",
    "                  dynamic_axes={'target' : {1: 'context_length'}, \n",
    "                                'target_mask' : {2: 'dim_mask',3: 'context_length'},\n",
    "                                'output' : {1: 'context_length'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load(\"model_1.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-4.0088654 -2.495728  -7.895707  ... -7.9997063 -3.8217719  2.9766543]\n",
      "  [-4.0081205 -2.4983144 -7.8962245 ... -7.99987   -3.823112   2.9746547]\n",
      "  [-4.0067267 -2.50055   -7.8969693 ... -8.000668  -3.8238568  2.973314 ]\n",
      "  ...\n",
      "  [-4.007272  -2.5028355 -7.897967  ... -8.002458  -3.8189769  2.9797707]\n",
      "  [-4.007879  -2.5008245 -7.8992763 ... -8.004711  -3.818676   2.9803545]\n",
      "  [-4.0080423 -2.4979782 -7.899762  ... -8.005637  -3.8176394  2.9818041]]]\n",
      "Exported model has been tested with ONNXRuntime, and the result looks good!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ort_session = ort.InferenceSession(\"model_1.onnx\", providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy()if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "# compute ONNX Runtime output prediction\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(src).astype('int64') ,\n",
    "              ort_session.get_inputs()[1].name: to_numpy(tgt).astype('int64'),\n",
    "              ort_session.get_inputs()[2].name: to_numpy(src_mask).astype('bool'),\n",
    "              ort_session.get_inputs()[3].name: to_numpy(tgt_mask).astype('bool')}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "print(ort_outs[0])\n",
    "\n",
    "# compare ONNX Runtime and PyTorch results\n",
    "np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)\n",
    "\n",
    "print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignore MatMul due to non constant B: /[/encoder/layers.0/mha/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/encoder/layers.0/mha/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/encoder/layers.1/mha/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/encoder/layers.1/mha/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/encoder/layers.2/mha/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/encoder/layers.2/mha/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/encoder/layers.3/mha/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/encoder/layers.3/mha/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/encoder/layers.4/mha/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/encoder/layers.4/mha/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/encoder/layers.5/mha/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/encoder/layers.5/mha/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/decoder/layers.0/mmha/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/decoder/layers.0/mmha/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/decoder/layers.0/mha/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/decoder/layers.0/mha/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/decoder/layers.1/mmha/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/decoder/layers.1/mmha/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/decoder/layers.1/mha/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/decoder/layers.1/mha/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/decoder/layers.2/mmha/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/decoder/layers.2/mmha/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/decoder/layers.2/mha/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/decoder/layers.2/mha/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/decoder/layers.3/mmha/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/decoder/layers.3/mmha/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/decoder/layers.3/mha/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/decoder/layers.3/mha/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/decoder/layers.4/mmha/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/decoder/layers.4/mmha/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/decoder/layers.4/mha/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/decoder/layers.4/mha/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/decoder/layers.5/mmha/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/decoder/layers.5/mmha/MatMul_1]\n",
      "Ignore MatMul due to non constant B: /[/decoder/layers.5/mha/MatMul]\n",
      "Ignore MatMul due to non constant B: /[/decoder/layers.5/mha/MatMul_1]\n"
     ]
    }
   ],
   "source": [
    "quantize_dynamic(\"model_1.onnx\", \"model_1_quantized.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchFile",
     "evalue": "[ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from model_1_quantized.onnx failed:Load model model_1_quantized.onnx failed. File doesn't exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchFile\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ort_session_2 \u001b[38;5;241m=\u001b[39m \u001b[43mort\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInferenceSession\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_1_quantized.onnx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m tgt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1210\u001b[39m,(batch_size, \u001b[38;5;241m1\u001b[39m),dtype\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mint64)\n\u001b[1;32m      3\u001b[0m tgt_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((batch_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbool)\n",
      "File \u001b[0;32m~/createch/IA/A5/wite_me_a_poeme/.venv/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:419\u001b[0m, in \u001b[0;36mInferenceSession.__init__\u001b[0;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m disabled_optimizers \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisabled_optimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisabled_optimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 419\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_inference_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproviders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisabled_optimizers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "File \u001b[0;32m~/createch/IA/A5/wite_me_a_poeme/.venv/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:452\u001b[0m, in \u001b[0;36mInferenceSession._create_inference_session\u001b[0;34m(self, providers, provider_options, disabled_optimizers)\u001b[0m\n\u001b[1;32m    450\u001b[0m session_options \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sess_options \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sess_options \u001b[38;5;28;01melse\u001b[39;00m C\u001b[38;5;241m.\u001b[39mget_default_session_options()\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_path:\n\u001b[0;32m--> 452\u001b[0m     sess \u001b[38;5;241m=\u001b[39m \u001b[43mC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInferenceSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_config_from_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    454\u001b[0m     sess \u001b[38;5;241m=\u001b[39m C\u001b[38;5;241m.\u001b[39mInferenceSession(session_options, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_bytes, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_config_from_model)\n",
      "\u001b[0;31mNoSuchFile\u001b[0m: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from model_1_quantized.onnx failed:Load model model_1_quantized.onnx failed. File doesn't exist"
     ]
    }
   ],
   "source": [
    "\n",
    "ort_session_2 = ort.InferenceSession(\"model_1_quantized.onnx\")\n",
    "tgt = torch.randint(0,1210,(batch_size, 1),dtype= torch.int64)\n",
    "tgt_mask = torch.ones((batch_size, 1, 1, 1), dtype= torch.bool)\n",
    "print(src, src_mask, tgt, tgt_mask)\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy()if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "# compute ONNX Runtime output prediction\n",
    "ort_inputs_2 = {ort_session_2.get_inputs()[0].name: to_numpy(src).astype('int64') ,\n",
    "              ort_session_2.get_inputs()[1].name: to_numpy(tgt).astype('int64'),\n",
    "              ort_session_2.get_inputs()[2].name: to_numpy(src_mask).astype('bool'),\n",
    "              ort_session_2.get_inputs()[3].name: to_numpy(tgt_mask).astype('bool')}\n",
    "ort_outs = ort_session_2.run(None, ort_inputs_2)\n",
    "print(ort_outs[0])\n",
    "\n",
    "# compare ONNX Runtime and PyTorch results\n",
    "# np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)\n",
    "\n",
    "print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
