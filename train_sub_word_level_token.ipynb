{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from new_transformer import Transformer\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "import tqdm\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPE_Tokenizer:\n",
    "    def _init_(self):\n",
    "        self.vocab = set()\n",
    "        self.token_to_index = {}\n",
    "        self.index_to_token = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def get_stats(vocab):\n",
    "        pairs = {}\n",
    "        for word, freq in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols)-1):\n",
    "                pair = (symbols[i], symbols[i+1])\n",
    "                if pair in pairs:\n",
    "                    pairs[pair] += freq\n",
    "                else:\n",
    "                    pairs[pair] = freq\n",
    "        return pairs\n",
    "\n",
    "    @staticmethod\n",
    "    def merge_vocab(pair, v_in):\n",
    "        v_out = {}\n",
    "        bigram = ' '.join(pair)\n",
    "        replacement = ''.join(pair)\n",
    "        for word in v_in:\n",
    "            w_out = word.replace(bigram, replacement)\n",
    "            v_out[w_out] = v_in[word]\n",
    "        return v_out\n",
    "\n",
    "    @staticmethod\n",
    "    def get_vocab(text):\n",
    "        vocab = {}\n",
    "        for word in text.split():\n",
    "            word = ' '.join(list(word)) +  ' </w>'\n",
    "            if word in vocab:\n",
    "                vocab[word] += 1\n",
    "            else:\n",
    "                vocab[word] = 1\n",
    "        return vocab\n",
    "\n",
    "    def train_until(self, text, vocab_size):\n",
    "        vocab = self.get_vocab(text)\n",
    "        self.vocab = set(word for word in vocab for word in word.split())\n",
    "        # Check if the initial vocabulary size is less than the desired size\n",
    "        while len(self.vocab) < vocab_size:\n",
    "            pairs = self.get_stats(vocab)\n",
    "            if not pairs:\n",
    "                break\n",
    "            best = max(pairs, key=pairs.get)\n",
    "            vocab = self.merge_vocab(best, vocab)\n",
    "            self.vocab = set(word for word in vocab for word in word.split())\n",
    "\n",
    "        # Add special tokens\n",
    "        self.vocab.add('</u>')\n",
    "        self.vocab.add('[]')\n",
    "\n",
    "        self.build_index()\n",
    "\n",
    "    def train(self, text, num_merges):\n",
    "            vocab = self.get_vocab(text)\n",
    "            for i in range(num_merges):\n",
    "                pairs = self.get_stats(vocab)\n",
    "                if not pairs:\n",
    "                    break\n",
    "                best = max(pairs, key=pairs.get)\n",
    "                vocab = self.merge_vocab(best, vocab)\n",
    "\n",
    "            self.vocab = set(word for word in vocab for word in word.split())\n",
    "\n",
    "            # Add special tokens\n",
    "            self.vocab.add('</u>')\n",
    "\n",
    "            self.build_index()\n",
    "\n",
    "    def build_index(self):\n",
    "        # existing code\n",
    "        self.token_to_index = {token: index for index, token in enumerate(self.vocab)}\n",
    "        self.index_to_token = {index: token for token, index in self.token_to_index.items()}\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        tokens = []\n",
    "        for word in text.split():\n",
    "            subwords = self.get_subwords(word + '</w>')\n",
    "            tokens.extend(self.token_to_index.get(sw, self.token_to_index['</u>']) for sw in subwords)\n",
    "        return tokens\n",
    "\n",
    "    def get_subwords(self, word):\n",
    "        subwords = []\n",
    "        while word:\n",
    "            subword = self.find_longest_subword(word)\n",
    "            if subword is None:\n",
    "                subwords.append('</u>')\n",
    "                break\n",
    "            subwords.append(subword)\n",
    "            word = word[len(subword):]\n",
    "        return subwords\n",
    "\n",
    "    def find_longest_subword(self, word):\n",
    "        for i in range(len(word), 0, -1):\n",
    "            if word[:i] in self.vocab:\n",
    "                return word[:i]\n",
    "        return None\n",
    "\n",
    "    def detokenize(self, token_ids):\n",
    "        words = []\n",
    "        current_word = ''\n",
    "        for token_id in token_ids:\n",
    "            token = self.index_to_token.get(token_id, '</u>')\n",
    "            if token == '</w>':\n",
    "                words.append(current_word)\n",
    "                current_word = ''\n",
    "            else:\n",
    "                current_word += token\n",
    "        words.append(current_word)\n",
    "        return ' '.join(words).replace('</w>', ' ')\n",
    "    \n",
    "    def add_special_tokens(self, special_tokens):\n",
    "        for token in special_tokens:\n",
    "            self.vocab.add(token)\n",
    "        self.build_index()\n",
    "\n",
    "    def save_vocab(self, file_path):\n",
    "        with open(file_path, 'w') as f:\n",
    "            json.dump(self.token_to_index, f)\n",
    "\n",
    "    def load_vocab(self, file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            self.token_to_index = json.load(f)\n",
    "            self.index_to_token = {int(index): token for token, index in self.token_to_index.items()}\n",
    "            self.vocab = set(self.token_to_index.keys())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.token_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text length: 31353581\n",
      "Training...\n",
      "Vocab: {'loren': 0, 'sthe</w>': 1, 'silis</w>': 2, 'x</w>': 3, 'ored</w>': 4, 'entiles</w>': 5, 'rento</w>': 6, 'iline</w>': 7, 'renou': 8, 'chis</w>': 9, 'atin': 10, 'ano</w>': 11, 'lilo': 12, 'oreth': 13, 'esun</w>': 14, 'al</w>': 15, 'aroi': 16, 'ly</w>': 17, 'ori</w>': 18, 'pout</w>': 19, 'ereti': 20, 'orel</w>': 21, 'norent</w>': 22, 'aporis</w>': 23, 'poreu': 24, 'arenti': 25, 'poinon</w>': 26, 'loron': 27, 'rine</w>': 28, 'atine</w>': 29, 'areri': 30, 'orine</w>': 31, 'orent</w>': 32, 'arente</w>': 33, 'entil</w>': 34, 'alo': 35, 'one</w>': 36, 'dinous</w>': 37, 'he</w>': 38, 'our</w>': 39, 'relon': 40, ',</w>': 41, 'erec': 42, 'loit</w>': 43, 'areth</w>': 44, 'nono': 45, 'reles</w>': 46, 'eles</w>': 47, 'arech</w>': 48, 'gh': 49, 'w': 50, 'recom': 51, 'sun</w>': 52, 'ail</w>': 53, 'rec': 54, 'ecom': 55, '/': 56, 'ghis</w>': 57, 'esil</w>': 58, 'areti': 59, 'y</w>': 60, 'ne</w>': 61, 'enof</w>': 62, 'nor': 63, 'dit</w>': 64, 'ari</w>': 65, 'orim': 66, 'onorer</w>': 67, 'inor': 68, 'norer': 69, 'restin': 70, 'onom': 71, 'enthat</w>': 72, 'ailor</w>': 73, 'nous</w>': 74, 'eresi': 75, 'whe</w>': 76, 'apour</w>': 77, 'tilo': 78, 'alone</w>': 79, 'elo': 80, 'poring</w>': 81, 'er': 82, 'rout</w>': 83, 'reti': 84, 'oun</w>': 85, 'loring</w>': 86, 'li</w>': 87, 'rely</w>': 88, 'dil</w>': 89, 'p': 90, 'C': 91, 'orily</w>': 92, 'athe</w>': 93, 'sin</w>': 94, 'tilis</w>': 95, 'con': 96, 'eret': 97, 'arine</w>': 98, 'oiles</w>': 99, 'anou</w>': 100, 'lorin</w>': 101, 'er</w>': 102, 'eril': 103, 'erili': 104, 'erev': 105, 'stim': 106, 'ar': 107, 'lun</w>': 108, 'ed</w>': 109, 'alou</w>': 110, 'astin': 111, 'oreline</w>': 112, 'atiles</w>': 113, 'orin': 114, 'am': 115, 'ered</w>': 116, 'arono</w>': 117, 'esu': 118, 'alu': 119, 'oril': 120, 'aime</w>': 121, 'ring</w>': 122, 'lil': 123, 'anof</w>': 124, 'arene</w>': 125, 'dinou</w>': 126, 'reme</w>': 127, 'rori': 128, 'is</w>': 129, 'lori</w>': 130, 'ili': 131, 'th': 132, 'eril</w>': 133, 'pores</w>': 134, 'tino': 135, 'enor</w>': 136, 'D': 137, 'sinore</w>': 138, 'ine</w>': 139, 'ilou</w>': 140, 'onorer': 141, 'aporer</w>': 142, 'stis</w>': 143, 'ene</w>': 144, 'eroit</w>': 145, 'erin': 146, 'ame</w>': 147, 'erone</w>': 148, 'rilo': 149, 'erono': 150, 'orev': 151, 'entili': 152, 'rili': 153, 'estis</w>': 154, 'renous</w>': 155, 'alon</w>': 156, 'ores</w>': 157, '8': 158, 'arent': 159, 'lit</w>': 160, 'econom': 161, 'eli</w>': 162, 'noreri': 163, 'retin': 164, 'atili': 165, 'tili': 166, '4': 167, 'esthat</w>': 168, 'poiling</w>': 169, 'im': 170, 'acon': 171, 'rene</w>': 172, 'ale</w>': 173, 'anous</w>': 174, 'rin</w>': 175, 'asin': 176, 'lone</w>': 177, 'J': 178, 'rel</w>': 179, 'que</w>': 180, 'rom': 181, 'com': 182, 'alou': 183, 'orelo': 184, 'atilin': 185, 'onor</w>': 186, 'apo': 187, 'aling</w>': 188, 'sing</w>': 189, 'aro': 190, 'rete</w>': 191, 'apores</w>': 192, 'lorentine</w>': 193, 'ai</w>': 194, 'tilit</w>': 195, 'qut</w>': 196, 'was</w>': 197, 'areu': 198, 'a': 199, 'l</w>': 200, 'c': 201, 'ois</w>': 202, 'n': 203, 'lin</w>': 204, 'ut</w>': 205, 'onor': 206, 'rech': 207, 'esino</w>': 208, 'arino</w>': 209, 'aloi</w>': 210, 'eu': 211, 'lois</w>': 212, 'apon</w>': 213, 'onou': 214, 'anori': 215, 'du': 216, 'etis</w>': 217, 'esi': 218, 'nori': 219, 'erel</w>': 220, 'ashe</w>': 221, 'the</w>': 222, 'wh': 223, 'Q': 224, 'aret': 225, 'enous</w>': 226, 'arere</w>': 227, 'pou': 228, 'loris</w>': 229, 'lino</w>': 230, 'resinous</w>': 231, '[]': 232, '2': 233, 'anor</w>': 234, 'eresu': 235, 'lili': 236, 'orec': 237, 'reut</w>': 238, 'd': 239, 'rit</w>': 240, '6': 241, 'astili': 242, 'ailin': 243, 'eroine</w>': 244, 'orecom': 245, 'por': 246, 'eune</w>': 247, 'ely</w>': 248, 'asil</w>': 249, 'ati</w>': 250, 'resting</w>': 251, 'norino</w>': 252, 'asile</w>': 253, 'roine</w>': 254, 'conoit</w>': 255, 'reun</w>': 256, 'oi</w>': 257, 'oily</w>': 258, 'resin': 259, 'erethat</w>': 260, 'eret</w>': 261, 'este</w>': 262, 'erel': 263, 'k': 264, 'agh': 265, 'lo': 266, 'ase</w>': 267, 'ror</w>': 268, 'recon': 269, 'estime</w>': 270, 'asun</w>': 271, 'pone</w>': 272, 'elois</w>': 273, 'ente</w>': 274, 'esil': 275, 'astily</w>': 276, 'aren': 277, 'stiles</w>': 278, 'alino': 279, 'riles</w>': 280, 'ome</w>': 281, 'renti': 282, 'areno': 283, 'enou</w>': 284, 'elin': 285, 't': 286, 'ares': 287, 'estine</w>': 288, 'resin</w>': 289, 'arestin': 290, 'il': 291, '7': 292, 'norer</w>': 293, 'asine</w>': 294, 'siles</w>': 295, 'sit</w>': 296, 'ele</w>': 297, 'lorent</w>': 298, 'E': 299, 'anor': 300, 'aly</w>': 301, 'oiling</w>': 302, 'aili': 303, 'o': 304, 'V': 305, 'lo</w>': 306, 'alis</w>': 307, 'asil': 308, 'esin</w>': 309, 'tino</w>': 310, 'q': 311, 'ail': 312, 'ec': 313, 'e</w>': 314, 'entim': 315, 'dune</w>': 316, 'enore</w>': 317, 'onorerent</w>': 318, 'lore': 319, 'asi': 320, 'ait</w>': 321, 'po': 322, 'ilori</w>': 323, 'ailu': 324, 'lim': 325, 'sting</w>': 326, 'tiling</w>': 327, 'lorer': 328, 'elous</w>': 329, 'enom': 330, 'apori': 331, 'sinous</w>': 332, 'oil': 333, 'de</w>': 334, 'lin': 335, 'wit</w>': 336, 'nou</w>': 337, 'ro': 338, 'orest': 339, 'aut</w>': 340, 'ate</w>': 341, 'une</w>': 342, 'ais</w>': 343, 'estily</w>': 344, 'au': 345, 'en</w>': 346, 'onorou': 347, 'asthe</w>': 348, 'inori': 349, 'time</w>': 350, 'elon': 351, 'elino</w>': 352, '</u>': 353, 'eth</w>': 354, 'ece</w>': 355, 'etil</w>': 356, 'erom': 357, 'ste</w>': 358, 'oilin': 359, 'con</w>': 360, 'norin': 361, 'rorent</w>': 362, 'inoi': 363, 'ilom': 364, 'rily</w>': 365, 'renou</w>': 366, 'aine</w>': 367, 'dis</w>': 368, 'and</w>': 369, 'ast': 370, 'entit</w>': 371, 'I': 372, 'noi': 373, 'u': 374, 'nois</w>': 375, 'enor': 376, 'oin': 377, 'es': 378, 'ev': 379, 'oret': 380, 'por</w>': 381, 'ag': 382, 'eli': 383, 'ailor': 384, 'wis</w>': 385, 'alil': 386, 'eresim': 387, 'loreron': 388, 'relim': 389, 'o</w>': 390, 'ino': 391, 'oing</w>': 392, 'lorentin': 393, 'arone</w>': 394, 'econe</w>': 395, 'entile</w>': 396, 'ile</w>': 397, 'oret</w>': 398, '0': 399, 'etin</w>': 400, 'ero</w>': 401, 'pon</w>': 402, 'arily</w>': 403, 'ld</w>': 404, 'lor': 405, 'et': 406, 'ila</w>': 407, 'rer</w>': 408, 'ech</w>': 409, 'aron</w>': 410, 'arom': 411, 'an</w>': 412, 'astim': 413, 'reth': 414, 'arent</w>': 415, 'dut</w>': 416, 'erof</w>': 417, 'ent': 418, 'onores</w>': 419, 'rono': 420, 'ilon': 421, 'ald</w>': 422, 'rone</w>': 423, 'S': 424, 'in</w>': 425, 'ren</w>': 426, 'resh': 427, 'roin</w>': 428, 'ash': 429, 'lorin': 430, 'ore</w>': 431, 'dile</w>': 432, 'ereth</w>': 433, 'erem': 434, 'ereli': 435, '1': 436, 'erin</w>': 437, 'orer</w>': 438, 'ori': 439, 'noron': 440, 'oit</w>': 441, 'eror</w>': 442, 'erilou': 443, 'oron': 444, 'erer': 445, 'or</w>': 446, 'erenti': 447, 'ch': 448, 'asing</w>': 449, 'G': 450, 'loin': 451, 'relis</w>': 452, 'erent</w>': 453, 'oili': 454, 'relu': 455, 'orono</w>': 456, 'arec': 457, 'in': 458, 'onorous</w>': 459, 'sine</w>': 460, 'rerent</w>': 461, 'reres</w>': 462, 'estin</w>': 463, 'ding</w>': 464, 'on</w>': 465, 'asti': 466, 'res</w>': 467, 'x': 468, 'oren</w>': 469, 'enti</w>': 470, 'anoine</w>': 471, 'ap': 472, 'des</w>': 473, 'at</w>': 474, 'les</w>': 475, 'en': 476, 'pous</w>': 477, 'orem': 478, 'reron': 479, 's</w>': 480, 'econ': 481, 'poil': 482, 'apoi': 483, 'aing</w>': 484, 'linon': 485, 'enon': 486, 'atiline</w>': 487, 'W': 488, 'al': 489, 'inous</w>': 490, 'arome</w>': 491, 'onon</w>': 492, 'wing</w>': 493, 'oim': 494, 'sil': 495, 'rois</w>': 496, 'alom': 497, 'et</w>': 498, 'ari': 499, 'nore': 500, 'pour</w>': 501, 'enting</w>': 502, 'rentis</w>': 503, 'reli': 504, 'cone</w>': 505, 'oro</w>': 506, 'or': 507, 'ilon</w>': 508, 'inone</w>': 509, 'eun</w>': 510, 'tis</w>': 511, 'onorere': 512, 'R': 513, 'an': 514, 'anoi': 515, 'wiles</w>': 516, 'oron</w>': 517, 'lune</w>': 518, 'ag</w>': 519, 'ril': 520, 'ereto</w>': 521, 'asth': 522, 'atinous</w>': 523, 'orese</w>': 524, 'din': 525, 'dilu': 526, 'dun</w>': 527, 'stil</w>': 528, 'rinoi': 529, 'resu': 530, 'rese</w>': 531, 's': 532, 'X': 533, 'asiles</w>': 534, 'stin': 535, 'ilori': 536, '-': 537, 'onon': 538, 'enois</w>': 539, 'astile</w>': 540, 'elim': 541, 'erome</w>': 542, 'renonon': 543, 'je</w>': 544, 'rere': 545, 'sth': 546, 'li': 547, 'porel': 548, 'ild</w>': 549, 'non</w>': 550, 'dily</w>': 551, 'T': 552, 'roin': 553, 'asis</w>': 554, 'tinous</w>': 555, 'renti</w>': 556, 'lon</w>': 557, 'tit</w>': 558, 'y': 559, 'rile</w>': 560, 'd</w>': 561, 'orest</w>': 562, 'orit</w>': 563, '?': 564, 'anoin': 565, 'orero': 566, 'a</w>': 567, 'th</w>': 568, 'ali': 569, 'arenth': 570, 'sili': 571, 'di</w>': 572, 'loil</w>': 573, 'inon': 574, 'esi</w>': 575, 'eshe</w>': 576, 'reune</w>': 577, 'K': 578, 'ati': 579, 'esting</w>': 580, 'onore</w>': 581, 'nom': 582, 'sune</w>': 583, 'orethe</w>': 584, 'pou</w>': 585, 'oile</w>': 586, 'loi</w>': 587, 'arerent</w>': 588, 'eloi': 589, 'apore</w>': 590, 'lou</w>': 591, 'alor</w>': 592, 'onous</w>': 593, 'pore</w>': 594, 'out</w>': 595, 'asily</w>': 596, 'ris</w>': 597, 'wili': 598, 'di': 599, 'ere</w>': 600, 'reting</w>': 601, 'pom': 602, 'sinon</w>': 603, 'aro</w>': 604, 'erese</w>': 605, 'oilu': 606, 'ronou': 607, 'sin': 608, 'eut</w>': 609, '9': 610, 'oreti': 611, '</w>': 612, 'line</w>': 613, 'ilin</w>': 614, 'sino</w>': 615, 'onorine</w>': 616, 'ro</w>': 617, 'dim': 618, 'rest</w>': 619, 'iling</w>': 620, 'tin</w>': 621, 'oresti': 622, 'ce</w>': 623, 'arois</w>': 624, 'apon': 625, 'orecon': 626, 'eling</w>': 627, 'wi': 628, 'nor</w>': 629, 'eti': 630, 'rome</w>': 631, 'oreri</w>': 632, 'arethe</w>': 633, 'ath</w>': 634, 'arous</w>': 635, 'etit</w>': 636, 'ering</w>': 637, 'rino': 638, 'asi</w>': 639, 'erois</w>': 640, 'onour</w>': 641, 't</w>': 642, 'tin': 643, 'lon': 644, 'ri': 645, 'pori': 646, 'rorou</w>': 647, 'nome</w>': 648, 'atil</w>': 649, 'lilu': 650, 'din</w>': 651, 'es</w>': 652, 'entin</w>': 653, 'noin': 654, 'elone</w>': 655, 'esti</w>': 656, 'reling</w>': 657, 'ilu': 658, 'alori': 659, 'ronon</w>': 660, 'iloris</w>': 661, 'poinon': 662, 'enou': 663, 'ilin': 664, 'ril</w>': 665, 'stile</w>': 666, 'acom': 667, 'ad</w>': 668, 'ronome</w>': 669, 'erit</w>': 670, 'lori': 671, 'arin</w>': 672, 'enoil': 673, 'aune</w>': 674, 'inon</w>': 675, 'poil</w>': 676, 'aril': 677, 'non': 678, '\"': 679, 'entil': 680, 'stili': 681, 'eris</w>': 682, 'asili': 683, 'ono</w>': 684, 'rent</w>': 685, 'arem': 686, '*': 687, 'entinois</w>': 688, 'oreron': 689, 'ri</w>': 690, 'relit</w>': 691, 'aporent</w>': 692, 'sim': 693, 'ting</w>': 694, 'si</w>': 695, 'arer': 696, 'inor</w>': 697, 'lout</w>': 698, 'atis</w>': 699, 'as</w>': 700, 'ales</w>': 701, 'roi': 702, 'entime</w>': 703, 'dino</w>': 704, 'nores': 705, 'til</w>': 706, 'M': 707, 'pome</w>': 708, 'la</w>': 709, 'ch</w>': 710, 'ano': 711, 'rorem': 712, 'ono': 713, 'st': 714, 'aring</w>': 715, 'ares</w>': 716, 'lino': 717, 'lily</w>': 718, 'asino</w>': 719, 'ereno': 720, 'win': 721, 'onored</w>': 722, 'erere</w>': 723, 'lored</w>': 724, 'oro': 725, 'areme</w>': 726, 'rore</w>': 727, 'em': 728, 'ere': 729, 'i': 730, 'nored</w>': 731, 'enoit</w>': 732, 'eres': 733, 'anon</w>': 734, 'aun</w>': 735, 'aloi': 736, 'ero': 737, 'inou</w>': 738, 'siloin': 739, 'elin</w>': 740, 'norim': 741, 'eld</w>': 742, 'asin</w>': 743, 'orer': 744, 'poin': 745, 'ali</w>': 746, 'erine</w>': 747, 'rour</w>': 748, 'renom': 749, 'anon': 750, 'erer</w>': 751, 'asu': 752, 'atime</w>': 753, 'esin': 754, 'achis</w>': 755, 'stin</w>': 756, 'porous</w>': 757, 'oreli': 758, 'aporous</w>': 759, 'estin': 760, 'r': 761, 'rel': 762, 'ou': 763, 'renon': 764, 'ace</w>': 765, 'alile</w>': 766, 'ecome</w>': 767, 'reste</w>': 768, 'erino</w>': 769, 'reli</w>': 770, 'w</w>': 771, 'ing</w>': 772, 'alim': 773, 'ronoun</w>': 774, 'atil': 775, 'athis</w>': 776, 'alut</w>': 777, 'arese</w>': 778, 'ilo</w>': 779, 'ime</w>': 780, 'f': 781, 'ac': 782, 'arerent': 783, 'etime</w>': 784, 'reri': 785, 'ainous</w>': 786, 'eror': 787, 'rest': 788, 'reto</w>': 789, 'lorine</w>': 790, 'sh': 791, 'rou': 792, 'lour</w>': 793, 'oi': 794, 'orin</w>': 795, 'rime</w>': 796, 'rela</w>': 797, 'ilo': 798, 'ti': 799, 'el</w>': 800, 'diles</w>': 801, 'ese</w>': 802, 'rino</w>': 803, 'noi</w>': 804, 'pon': 805, 'esthe</w>': 806, 'atile</w>': 807, 'astil': 808, 'lil</w>': 809, 'ores': 810, 'aresi': 811, 'alour</w>': 812, 'tiles</w>': 813, 'resi': 814, 'come</w>': 815, 'te</w>': 816, 'orou': 817, 'anour</w>': 818, 'eno</w>': 819, 'arel': 820, 'conom': 821, 'eroin': 822, 'oila</w>': 823, 'lut</w>': 824, 'st</w>': 825, 'enti': 826, 'lou': 827, 'dine</w>': 828, 'alor': 829, 're</w>': 830, 'z': 831, 'eroi</w>': 832, 'erou': 833, 'arere': 834, 'dilo': 835, 'at': 836, 'rem': 837, 'are</w>': 838, 'stil': 839, 'dil': 840, 'inou': 841, 'ain': 842, 'eres</w>': 843, 'to</w>': 844, 'esth': 845, 'elout</w>': 846, 'lor</w>': 847, 'lis</w>': 848, 'rononon': 849, 'erous</w>': 850, 'resti': 851, 'aim': 852, 'tile</w>': 853, 'aril</w>': 854, 'esti': 855, 'astis</w>': 856, 'elor': 857, 'Y': 858, 'reu': 859, 'it</w>': 860, 'wily</w>': 861, 'roring</w>': 862, 'L': 863, 'lof</w>': 864, 'elof</w>': 865, 'esine</w>': 866, 'silu': 867, 'reche</w>': 868, 'ento</w>': 869, 'inorit</w>': 870, 'rou</w>': 871, 'U': 872, 'relin': 873, 'si': 874, 'f</w>': 875, 'O': 876, 'arethat</w>': 877, 'alit</w>': 878, 'asting</w>': 879, 'aroi</w>': 880, 'esh': 881, 'anou': 882, '5': 883, 'asthat</w>': 884, 'ret': 885, 'ared</w>': 886, 'enth</w>': 887, 'ete</w>': 888, 'sis</w>': 889, 'v': 890, 'noring</w>': 891, 'orous</w>': 892, 'me</w>': 893, 'ronon': 894, 'noun</w>': 895, 'alo</w>': 896, 'ailes</w>': 897, 'arin': 898, 'lore</w>': 899, 'rentime</w>': 900, 'onorit</w>': 901, 'no</w>': 902, 'rilit</w>': 903, 'sut</w>': 904, 'asto</w>': 905, 'g': 906, 'eto</w>': 907, 'pof</w>': 908, 'sto</w>': 909, 'onorent</w>': 910, 'erily</w>': 911, 'ino</w>': 912, 'apor': 913, 'alorou': 914, 'this</w>': 915, 'eting</w>': 916, 'loro': 917, 'stily</w>': 918, 'lu': 919, 'arest</w>': 920, 'arech': 921, 'win</w>': 922, 'ereu': 923, 'eno': 924, 'po</w>': 925, 'elon</w>': 926, 'ethis</w>': 927, 'sti</w>': 928, 'b': 929, 'alin</w>': 930, 'sime</w>': 931, 'til': 932, 'oring</w>': 933, 're': 934, 'erilous</w>': 935, 'atit</w>': 936, 'ala</w>': 937, 'conome</w>': 938, 'are': 939, 'ereth': 940, 'eche</w>': 941, 'acon</w>': 942, 'ror': 943, '3': 944, 'roit</w>': 945, 'oilune</w>': 946, 'e': 947, 'onoi': 948, 'che</w>': 949, 'tilu': 950, 'F': 951, 'ous</w>': 952, 'erest</w>': 953, 'enoi': 954, 'ron': 955, 'aile</w>': 956, 'ilor': 957, 'none</w>': 958, 'orere': 959, 'etim': 960, 'tim': 961, 'il</w>': 962, 'loi': 963, 'ethat</w>': 964, 'ato</w>': 965, 'onori': 966, 'om': 967, 'un</w>': 968, 'oren': 969, 'j': 970, 'erou</w>': 971, 'eretil': 972, 'ilous</w>': 973, 'eroi': 974, 'entine</w>': 975, 'she</w>': 976, '!</w>': 977, 'wil': 978, 'stine</w>': 979, 'red</w>': 980, 'roiling</w>': 981, 'orerent</w>': 982, 'rere</w>': 983, 'ilis</w>': 984, 'esing</w>': 985, 'alun</w>': 986, 'orely</w>': 987, 'erentil': 988, 'arit</w>': 989, 'rero': 990, 'porel</w>': 991, 'lome</w>': 992, 'reno': 993, 'aris</w>': 994, 'rent': 995, 'his</w>': 996, 'oreune</w>': 997, 'r</w>': 998, 'astime</w>': 999, 'erecom': 1000, 'elit</w>': 1001, 'ti</w>': 1002, 'oreri': 1003, 'ath': 1004, 'eti</w>': 1005, 'P': 1006, 'erech': 1007, 'ab': 1008, 'areron': 1009, 'sil</w>': 1010, 'ret</w>': 1011, 'erile</w>': 1012, 'rorer</w>': 1013, 'alon': 1014, 'alous</w>': 1015, 'sthat</w>': 1016, 'oreu': 1017, 'onoring</w>': 1018, 'elis</w>': 1019, 'oresi': 1020, 'alin': 1021, 'ech': 1022, 'wim': 1023, 'ilim': 1024, 'nout</w>': 1025, 'ailing</w>': 1026, 'ans</w>': 1027, 'erent': 1028, 'roris</w>': 1029, 'B': 1030, 'eth': 1031, 'orone</w>': 1032, 'eloin': 1033, 'est': 1034, 'rim': 1035, 'orel': 1036, 'entis</w>': 1037, 'orom': 1038, 'arel</w>': 1039, 'h': 1040, 'dime</w>': 1041, 'lom': 1042, 'lorous</w>': 1043, 'conois</w>': 1044, 'inois</w>': 1045, 'etil': 1046, 'eline</w>': 1047, 'relut</w>': 1048, 'relo': 1049, 'ilout</w>': 1050, 'tilin': 1051, 'ronom': 1052, 'esile</w>': 1053, 'estim': 1054, 'eme</w>': 1055, 'resi</w>': 1056, 'ling</w>': 1057, 'lorentin</w>': 1058, 'ore': 1059, 'as': 1060, 'on': 1061, 'lous</w>': 1062, 'nore</w>': 1063, 'poi': 1064, 'sily</w>': 1065, 'ethe</w>': 1066, 'oris</w>': 1067, 'atin</w>': 1068, 'poing</w>': 1069, 'H': 1070, 'esit</w>': 1071, 'rero</w>': 1072, 'of</w>': 1073, 'eron</w>': 1074, 'ating</w>': 1075, 'erely</w>': 1076, 'eron': 1077, 'asim': 1078, 'N': 1079, 'orech': 1080, 'iles</w>': 1081, 'se</w>': 1082, '.</w>': 1083, 'g</w>': 1084, 'alois</w>': 1085, 'alila</w>': 1086, 'elune</w>': 1087, 'no': 1088, 'tine</w>': 1089, 'pored</w>': 1090, 'aporeu': 1091, 'ent</w>': 1092, 'ane</w>': 1093, 'eren': 1094, 'tily</w>': 1095, 'arer</w>': 1096, 'ela</w>': 1097, 'ilit</w>': 1098, 'esis</w>': 1099, 'aline</w>': 1100, 'm': 1101, 'erene</w>': 1102, 'elu': 1103, 'wild</w>': 1104, 'rev': 1105, 'elor</w>': 1106, 'rored</w>': 1107, 'ast</w>': 1108, 'aret</w>': 1109, 'est</w>': 1110, 'lime</w>': 1111, 'oine</w>': 1112, \"'\": 1113, 'ache</w>': 1114, 'oin</w>': 1115, 'rer': 1116, 'dili': 1117, 'linois</w>': 1118, 'entin': 1119, 'ren': 1120, 'elil': 1121, 'erestin': 1122, 'stino</w>': 1123, 'el': 1124, 'ach</w>': 1125, 'roil</w>': 1126, 'ron</w>': 1127, 'inorin': 1128, 'sti': 1129, 'that</w>': 1130, '`': 1131, 'shis</w>': 1132, 'eri': 1133, 'enth': 1134, 'nou': 1135, 'ach': 1136, 'anois</w>': 1137, 'rele</w>': 1138, 'roi</w>': 1139, 'loin</w>': 1140, 'eresh': 1141, 'roil': 1142, 'rin': 1143, 'oreno': 1144, 'lores': 1145, 'ili</w>': 1146, 'rece</w>': 1147, 'apou': 1148, 'I</w>': 1149, 'av': 1150, 'enon</w>': 1151, 'dino': 1152, 'ou</w>': 1153, 'aste</w>': 1154, 'oroi': 1155, 'anout</w>': 1156, 'onof</w>': 1157, 'ailoring</w>': 1158, 'A': 1159, 'elou': 1160, 'erim': 1161, 'le</w>': 1162, 'ily</w>': 1163, 'stilit</w>': 1164, 'wine</w>': 1165, 'anom': 1166, 'eri</w>': 1167, 'pore': 1168, 'lorer</w>': 1169, 'ain</w>': 1170, 'elo</w>': 1171, 'etin': 1172, 'aron': 1173, 'onore': 1174, 'rous</w>': 1175, 'ant</w>': 1176, 'estil': 1177, 'eresting</w>': 1178, 'ai': 1179, 'arou': 1180, 'apor</w>': 1181, 'aily</w>': 1182, 'rente</w>': 1183, 'res': 1184, 'porer</w>': 1185, 'l': 1186, 'nores</w>': 1187, 'ar</w>': 1188, 'qu': 1189, 'reren': 1190, 'lores</w>': 1191, 'erecon': 1192, 'pois</w>': 1193, 'arely</w>': 1194, 'Z': 1195, 'ereste</w>': 1196, 'stime</w>': 1197, 'n</w>': 1198, 'esim': 1199, 'sile</w>': 1200, 'esto</w>': 1201, 'erest': 1202, 'arete</w>': 1203, 'su': 1204, 'oil</w>': 1205, 'i</w>': 1206}\n",
      "vocab size: 1207\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import tqdm as tqdm\n",
    "import pandas as pd\n",
    "dataset = datasets.load_dataset(\"opus_books\", \"en-fr\")\n",
    "en_content = ''.join([dataset[\"train\"][i][\"translation\"][\"en\"] for i in range(dataset.num_rows[\"train\"])])\n",
    "fr_content = ''.join([dataset[\"train\"][i][\"translation\"][\"fr\"] for i in range(dataset.num_rows[\"train\"])])\n",
    "global_content = en_content + fr_content\n",
    "\n",
    "tokenizer = BPE_Tokenizer()\n",
    "\n",
    "num_merges =  90\n",
    "untils = []\n",
    "total_tokens = []\n",
    "avg_token_len = []\n",
    "avg_token_std = []\n",
    "\n",
    "def clean(text: str) -> str:\n",
    "    text = text.replace('.', ' . ')\n",
    "    text = text.replace(',', ' , ')\n",
    "    text = text.replace('!', ' ! ')\n",
    "    text = text.replace('?', ' ? ')\n",
    "    text = text.replace(':', ' : ')\n",
    "    text = text.replace(';', ' ; ')\n",
    "    text = text.replace(')', '')\n",
    "    text = text.replace('(', '')\n",
    "    text = text.replace('@', '')\n",
    "    text = text.replace('|', '')\n",
    "    text = text.replace(']', '')\n",
    "    text = text.replace('[', '')\n",
    "    text = text.replace('~', '')\n",
    "    text = text.replace('^', '')\n",
    "    text = text.replace('<', '')\n",
    "    text = text.replace('>', '')\n",
    "    text = text.replace('&', '')\n",
    "    text = text.replace('{', '')\n",
    "    text = text.replace('}', '')\n",
    "    text = text.replace('+', '')\n",
    "    # text = text.replace('-', '')\n",
    "    text = text.replace('tititi', '')\n",
    "    text = text.replace('orerer', '')\n",
    "    text = text.replace('errero', '')\n",
    "    text = text.replace('\\u007f', '')\n",
    "    text = text.replace('_', '')\n",
    "    text = text.replace('%', '')\n",
    "    text = text.replace('$', '')\n",
    "    text = text.replace('\\\\', '')\n",
    "    text = text.replace('=', '')\n",
    "    text = text.replace('#', '')\n",
    "    text = text.replace(';', '')\n",
    "    text = text.replace(':', '')\n",
    "    text = text.encode(\"ascii\", errors=\"ignore\").decode()\n",
    "    return text\n",
    "\n",
    "\n",
    "global_content = clean(global_content)\n",
    "print(\"text length:\", len(global_content))\n",
    "print(\"Training...\")\n",
    "tokenizer.train_until(global_content, 1200)\n",
    "# tokenizer.load_vocab('token_to_index.json')\n",
    "\n",
    "print(\"Vocab:\", tokenizer.token_to_index)\n",
    "print(\"vocab size:\", len(tokenizer))\n",
    "\n",
    "\n",
    "tokenizer.add_special_tokens(['<PAD></w>', '<SOS></w>','<EOS></w>'])\n",
    "\n",
    "\n",
    "# Saving token_to_index mapping\n",
    "tokenizer.save_vocab('en-fr.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [10, 645, 179, 315, 1207, 130, 973, 248, 399]\n",
      "num Tokens: 9\n",
      "Detokenized Text: <PAD> <SOS> Je suis jeune <EOS> \n"
     ]
    }
   ],
   "source": [
    "# Tokenizing\n",
    "token_ids = tokenizer.tokenize(\"<PAD> <SOS> Je suis jeune <EOS>\")\n",
    "print(\"Token IDs:\", token_ids)\n",
    "print(\"num Tokens:\", len(token_ids))\n",
    "\n",
    "# Detokenizing\n",
    "detokenized_text = tokenizer.detokenize(token_ids)\n",
    "print(\"Detokenized Text:\", detokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[645]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"<SOS>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilingualDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer: BPE_Tokenizer, seq_len) -> None:\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.sos_idx = torch.tensor(self.tokenizer.tokenize(\"<SOS>\"), dtype = torch.int64)\n",
    "        self.eos_idx = torch.tensor(self.tokenizer.tokenize(\"<EOS>\"), dtype = torch.int64)\n",
    "        self.pad_idx = torch.tensor(self.tokenizer.tokenize(\"<PAD>\"), dtype = torch.int64)\n",
    "        \n",
    "    def _causal_mask(self, seq_len: int) -> torch.Tensor:\n",
    "        mask = torch.ones(1, seq_len, seq_len, dtype=torch.bool)\n",
    "        mask = torch.tril(mask, diagonal=0)\n",
    "        return mask\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        src = torch.tensor(self.tokenizer.tokenize(clean(self.dataset[idx][\"en\"])), dtype = torch.int64)\n",
    "        tgt = torch.tensor(self.tokenizer.tokenize(clean(self.dataset[idx][\"fr\"])), dtype = torch.int64)\n",
    "        if self.seq_len - len(src) - 2 < 0 or self.seq_len - len(tgt) - 1 < 0:\n",
    "            src = src[:self.seq_len - 2]\n",
    "            tgt = tgt[:self.seq_len - 1]\n",
    "        enc_num_pad = self.seq_len - len(src) - 2\n",
    "        dec_num_pad = self.seq_len - len(tgt) - 1\n",
    "        input_src = torch.cat([self.sos_idx, src, self.eos_idx, self.pad_idx.repeat(enc_num_pad)])\n",
    "        input_tgt = torch.cat([self.sos_idx, tgt, self.pad_idx.repeat(dec_num_pad)])\n",
    "        input_label = torch.cat([tgt,self.eos_idx, self.pad_idx.repeat(dec_num_pad)])\n",
    "        return (\n",
    "            input_src, \n",
    "            input_tgt, \n",
    "            input_label, \n",
    "            (input_src!=self.pad_idx).unsqueeze(0).unsqueeze(0).int() == 1,\n",
    "            (input_tgt!=self.pad_idx).unsqueeze(0).unsqueeze(0).int() & self._causal_mask(self.seq_len) == 1,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [x for x in dataset[\"train\"][\"translation\"] if len(tokenizer.tokenize(x[\"en\"])) < 100 and len(tokenizer.tokenize(x[\"fr\"]))  < 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = dataset[:int(len(dataset)*0.8)]\n",
    "val_set = dataset[int(len(dataset)*0.8):int(len(dataset)*0.9)]\n",
    "test_set = dataset[int(len(dataset)*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_seq_len = 0\n",
    "# sum = 0\n",
    "# for i in range(len(train_set)):\n",
    "#     src = train_set[i][\"en\"]\n",
    "#     tgt = train_set[i][\"fr\"]\n",
    "#     max_seq_len = max(max_seq_len, len(tokenizer.tokenize(src)), len(tokenizer.tokenize(tgt)))\n",
    "#     sum+=len(tokenizer.tokenize(src))\n",
    "# mean = sum/len(train_set)\n",
    "# max_seq_len, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = BilingualDataset(train_set, tokenizer, 100)\n",
    "val_set = BilingualDataset(val_set, tokenizer, 100)\n",
    "test_set = BilingualDataset(test_set, tokenizer, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83537, 10442, 10443)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_train), len(val_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_src, input_tgt, input_label, src_mask, tgt_mask = dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100]),\n",
       " torch.Size([100]),\n",
       " torch.Size([100]),\n",
       " torch.Size([1, 1, 100]),\n",
       " torch.Size([1, 100, 100]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tgt.shape, input_src.shape, input_label.shape, src_mask.shape, tgt_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_head=6,\n",
    "    embed_size=600,\n",
    "    context_length=100,\n",
    "    dropout=0.1,\n",
    "    num_layers=6,\n",
    "    device=device,\n",
    ")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_epochs = 5\n",
    "lr = 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> rimetin rorionorous norinIt cinorinase \"arin reche o ast erinorelirime reroorelirime esting ast erinorelirime lois eremares asiliast erinorelirime lois eremares asiliast erinorelirime lois oreces enomerinorelilis ares asiliast erinorelilis ares asiliast erinorelilis ares asiliast erinorelilis ares asiliast erinorelilis ares asiliast erinorelilis ares asiliast erinorelilis ares ailinatile come arfpo arin oit ais lored eshais lored \n"
     ]
    }
   ],
   "source": [
    "def gen(model: nn.Module, sentence: str, max_len: int, vocab: BPE_Tokenizer, device: torch.device):\n",
    "    model.eval()\n",
    "    sos_token = torch.tensor(vocab.tokenize(\"<SOS>\"), dtype=torch.int64).to(device)\n",
    "    eos_token = torch.tensor(vocab.tokenize(\"<EOS>\"), dtype=torch.int64).to(device)\n",
    "    pad_token = torch.tensor(vocab.tokenize(\"<PAD>\"), dtype=torch.int64).to(device)\n",
    "    # print(sos_token, eos_token, pad_token)\n",
    "    \n",
    "    src_input = torch.cat([sos_token, torch.tensor(vocab.tokenize(sentence), dtype=torch.int64).to(device), eos_token, pad_token.repeat(max_len - len(vocab.tokenize(sentence)) - 2)])\n",
    "    src_mask = (src_input != pad_token).unsqueeze(0).int() == 1\n",
    "    \n",
    "    tgt_input = sos_token\n",
    "    while tgt_input[-1] != eos_token and len(tgt_input) < max_len:\n",
    "        tgt_mask = dataset_train._causal_mask(tgt_input.shape[0]) == 1\n",
    "        src_input, tgt_input, src_mask, tgt_mask = src_input.to(device), tgt_input.to(device), src_mask.to(device), tgt_mask.to(device)\n",
    "        # print(src_input.unsqueeze(0).shape, tgt_input.unsqueeze(0).shape, src_mask.unsqueeze(0).shape, tgt_mask.unsqueeze(0).shape)\n",
    "        # print(src_input.unsqueeze(0).dtype, tgt_input.unsqueeze(0).dtype, src_mask.unsqueeze(0).dtype, tgt_mask.unsqueeze(0).dtype)\n",
    "\n",
    "        logits = model(src_input.unsqueeze(0), tgt_input.unsqueeze(0), src_mask.unsqueeze(0), tgt_mask.unsqueeze(0))\n",
    "        pred = F.softmax(logits, dim=-1)\n",
    "        # print(pred.shape)\n",
    "        # print(pred[:, -1, :].argmax(dim=-1))\n",
    "        # next_token = torch.multinomial(pred[:,-1,:], num_samples=1)\n",
    "        tgt_input = torch.cat([tgt_input, pred[:,-1,:].argmax(dim = -1).to(device)])\n",
    "        # print(tgt_input)\n",
    "    print(vocab.detokenize(tgt_input.tolist()))\n",
    "gen(model, \"I am a student\", 100, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> All his life seemed to pass before his eyes . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "rimece res ilorPath pou eth alereshentil dinou enomaret eting sthe ach ron enomthe tin sufenometing shis tin pou rento aret aret aret rento rento aret rento red rento aret aret red aret aret aret aret pou aret aret rento aret aret pou pou esto pou aret rento aret pou rento rento aret aret aret aret rento rento aret aret aret aret aret rento aret aret aret aret rento rento oun aret aret aret aret rento rento aret tin aret aret aret rento aret aret rento rento rento aret aret aret \n",
      "Toute sa vie semblait tre passe dans ses yeux . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "test génération: I am a student ->\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss 7.463 | Acc 0.000: : 1it [00:00,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> rimetin rorionorous norinIt cinorinase \"arin reche o ast erinorelirime reroorelirime esting ast erinorelirime lois eremares asiliast erinorelirime lois eremares asiliast erinorelirime lois oreces enomerinorelilis ares asiliast erinorelilis ares asiliast erinorelilis ares asiliast erinorelilis ares asiliast erinorelilis ares asiliast erinorelilis ares asiliast erinorelilis ares ailinatile come arfpo arin oit ais lored eshais lored \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss 3.299 | Acc 0.560: : 3it [00:01,  2.71it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss 1.310 | Acc 0.685: : 1000it [04:28,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> No crewmen . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "Nin i meume . m'aui tin. <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "Aucun homme de l'quipage . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "test génération: I am a student ->\n",
      "<SOS> Je suis un mourt . <EOS> \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss 1.068 | Acc 0.729: : 2000it [08:59,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> It was of interest to me to hear these men , who were spending their lives in fighting against our neighbours , discussing their character and ways . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "I'vendqurand mstrt , es rr, es mommes , qui me ensaient teur pie , prtre , tpix s , et psant utes de pouttre de de dts s . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "Je prenais grand intrt couter ces hommes , qui passaient leur vie combattre nos voisins , en discuter le caractre et les mthodes . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "test génération: I am a student ->\n",
      "<SOS> Je suis une murmura <EOS> \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss 1.005 | Acc 0.752: : 2610it [11:44,  3.70it/s]\n",
      "Epoch 0 | Loss 0.897: 100%|██████████| 326/326 [00:28<00:00, 11.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1Validation Loss: 1.0352328117266854\n",
      "Validation Loss Decreased(inf--->10809.901020) \t Saving The Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> \"My Lord ! \" cried dArtagnan , enlightened by a sudden idea , \"my Lord ! Pardon me , monsieur , but you are not--\" <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "-- Mon d , s'cria d'Artagnan , s i ent , 'un fde , rite , mord , sendon, mais ceur , ais m--que vous touez v. <EOS> . <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "-- Milord ! s'cria d'Artagnan illumin d'une ide subite , Milord ! pardon , monsieur mais est-ce que vous seriez . . . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "test génération: I am a student ->\n",
      "<SOS> Je suis un mot <EOS> \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss 1.057 | Acc 0.742: : 1000it [04:30,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> \"Discussing the gipsy , I daresay . \" <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "-- Je le us, 'il peut d, la cuhmienne , <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "-- Je pense qu'on parle de la Bohmienne . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "test génération: I am a student ->\n",
      "<SOS> Je suis un tuds-huit <EOS> \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss 0.960 | Acc 0.759: : 2000it [09:00,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> The engineer examined this black granite . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "L'ingnieur ebscva cgrand de . r . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "L'ingnieur observa ce granit noir . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "test génération: I am a student ->\n",
      "<SOS> Je suis un pas <EOS> \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss 0.938 | Acc 0.764: : 2610it [11:45,  3.70it/s]\n",
      "Epoch 1 | Loss 0.880: 100%|██████████| 326/326 [00:28<00:00, 11.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2Validation Loss: 0.9074117751487757\n",
      "Validation Loss Decreased(10809.901020--->9475.193756) \t Saving The Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> \"I fear so . \" <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "-- Jh bien . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "-- Eh bien ! <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "test génération: I am a student ->\n",
      "<SOS> Je me redoutai <EOS> \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Loss 0.921 | Acc 0.762: : 1000it [04:30,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> The men had done all that men could do . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "Pa mises ers avaient t ait tout ce quon il s avvaient . aire . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "Les passagers avaient fait tout ce qu'ils pouvaient faire . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "test génération: I am a student ->\n",
      "<SOS> Je suis honteux <EOS> \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Loss 0.814 | Acc 0.791: : 2000it [09:00,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> \"Beat , is he ? \" answered Belcher . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "-omyez-vous ,  rBelcher . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "Croyez-vous ? dit Belcher . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "test génération: I am a student ->\n",
      "<SOS> Je suis un curie <EOS> \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Loss 0.631 | Acc 0.834: : 2610it [11:45,  3.70it/s]\n",
      "Epoch 2 | Loss 0.684: 100%|██████████| 326/326 [00:28<00:00, 11.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3Validation Loss: 0.847887329981259\n",
      "Validation Loss Decreased(9475.193756--->8853.639500) \t Saving The Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> Then , when the operation was over , we burned every trace of our stay on that islet , which if I could have , I'd have blown up . \" <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "Cenis , qu'avbration , ait re ine , si mav et t 'ouu , ait te la rav, la tre prosage , r lethb, nous amai rais pait poutant nopnous l'esais du . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "Puis , l'opration termine , le feu a dtruit toute trace de notre passage sur cet lot que j'aurais fait sauter , si je l'avais pu . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "test génération: I am a student ->\n",
      "<SOS> Je suis un couteau <EOS> \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Loss 0.677 | Acc 0.822: : 1000it [04:30,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> Ces rcits , ces rumeurs , prirent bientt un corps et , a force detre confirms cent fois , firent comprendre ce qui en tait . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "That cw, ware nbns , ook , ch banc, sop, whthere coudwly ned , conalfoudwly ded , whntil theiwpluve . em elves . to a meeorit. at. <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "These tales and rumours took substance and shape , and were corroborated and re-corroborated , until they resolved themselves into a definite name . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "test génération: I am a student ->\n",
      "<SOS> Je suis un cuisine <EOS> \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Loss 0.713 | Acc 0.817: : 2000it [09:00,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> As for the mother , he could not tell . . . He gave me long explanations as to the one friend of the family . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "Quant aumre , il ne 'ffirma pen de l ne ponnait lonlongues amxplications aume des x foul des i des la famille . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "Quant la mre , il naffirmait rien Il me donna de longues explications comme au seul ami de la famille . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "test génération: I am a student ->\n",
      "<SOS> Je suis un double <EOS> \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Loss 0.687 | Acc 0.827: : 2610it [11:44,  3.70it/s]\n",
      "Epoch 3 | Loss 0.829: 100%|██████████| 326/326 [00:28<00:00, 11.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4Validation Loss: 0.823269912918462\n",
      "Validation Loss Decreased(8853.639500--->8596.584431) \t Saving The Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> And he looked at her fixedly , while in his hand he held two long papers that he slid between his nails . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "En , se ttdra dorxement dpout ce trdil on lonain il eux longuil liers quil laisait srsser sre ses mgles . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "Et il la considrait fixement , tout en tenant sa main deux longs papiers quil faisait glisser entre ses ongles . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "test génération: I am a student ->\n",
      "<SOS> Je suis un frisson <EOS> \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Loss 0.664 | Acc 0.825: : 1000it [04:30,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> A definite drowsiness overcame us . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "Une fotaine inomme lennv'enpara de nous . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "Une certaine somnolence s'emparait de nous . <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "test génération: I am a student ->\n",
      "<SOS> Je suis un pas <EOS> \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Loss 0.651 | Acc 0.831: : 2000it [09:00,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> What sperm whales you're handing us ! <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "Quelle sachalots de vhalres ! <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "Quels cachalots que les vtres ! <EOS> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> \n",
      "test génération: I am a student ->\n",
      "<SOS> Je suis un habitude <EOS> \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Loss 0.795 | Acc 0.790: : 2610it [11:45,  3.70it/s]\n",
      "Epoch 4 | Loss 0.794: 100%|██████████| 326/326 [00:28<00:00, 11.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5Validation Loss: 0.806047816770649\n",
      "Validation Loss Decreased(8596.584431--->8416.751303) \t Saving The Model\n"
     ]
    }
   ],
   "source": [
    "min_valid_loss = np.inf\n",
    "for _ in range(num_epochs):\n",
    "    model.train()\n",
    "    with tqdm.tqdm(enumerate(train_loader)) as pbar:\n",
    "        for idx, (src, tgt, label, src_mask, tgt_mask) in pbar:\n",
    "            src = src.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "            label = label.to(device)\n",
    "            src_mask = src_mask.to(device)\n",
    "            tgt_mask = tgt_mask.to(device)\n",
    "            output = model(src, tgt, src_mask, tgt_mask)\n",
    "            B, T, C = output.shape\n",
    "            if idx%1000 == 0:\n",
    "                print(tokenizer.detokenize(src[0].tolist()))\n",
    "                print(tokenizer.detokenize(output.argmax(dim=-1)[0].tolist()))\n",
    "                print(tokenizer.detokenize(label[0].tolist()))\n",
    "                print(\"test génération: I am a student ->\")\n",
    "                gen(model, \"I am a student\", 100, tokenizer, device)\n",
    "                print(\"\\n\\n\")\n",
    "            loss = F.cross_entropy(output.view(B * T, C), label.view(B * T), ignore_index=tokenizer.tokenize(\"[PAD]\")[0])\n",
    "            acc = (output.argmax(dim=-1) == label).float().mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            pbar.set_description(f\"Epoch {_} | Loss {loss.item():.3f} | Acc {acc.item():.3f}\")\n",
    "            # break\n",
    "    valid_loss = 0\n",
    "    model.eval()\n",
    "    with tqdm.tqdm(val_loader) as pbar:\n",
    "        with torch.no_grad():\n",
    "            for src, tgt, label, src_mask, tgt_mask in pbar:\n",
    "                src = src.to(device)\n",
    "                tgt = tgt.to(device)\n",
    "                label = label.to(device)\n",
    "                src_mask = src_mask.to(device)\n",
    "                tgt_mask = tgt_mask.to(device)\n",
    "                \n",
    "                output = model(src, tgt, src_mask, tgt_mask)\n",
    "                loss = F.cross_entropy(output.view(B * T, C), label.view(B * T), ignore_index=tokenizer.tokenize(\"[PAD]\")[0])\n",
    "                valid_loss += loss.item()*src.shape[0]\n",
    "                pbar.set_description(f\"Epoch {_} | Loss {loss.item():.3f}\")\n",
    "    print(f'Epoch {_+1}Validation Loss: {valid_loss / len(val_set)}')\n",
    "    if min_valid_loss > valid_loss:\n",
    "        print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n",
    "        min_valid_loss = valid_loss\n",
    "        # Saving State Dict\n",
    "        torch.save(model.state_dict(), 'saved_model_tokenizer_3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (embed): Embedding(1210, 600)\n",
       "    (pos_encoding): SinusoidEncoding()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x Block(\n",
       "        (qkv): Linear(in_features=600, out_features=1800, bias=False)\n",
       "        (mha): MultiHeadAttention(\n",
       "          (fc_out): Linear(in_features=600, out_features=600, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (fc_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ffwd): FeedForward(\n",
       "          (0): Linear(in_features=600, out_features=2400, bias=False)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=2400, out_features=600, bias=False)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm1): LayerNorm((600,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((600,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embed): Embedding(1210, 600)\n",
       "    (pos_encoding): SinusoidEncoding()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x DecoderBlock(\n",
       "        (qkv): Linear(in_features=600, out_features=1800, bias=False)\n",
       "        (mmha): MultiHeadAttention(\n",
       "          (fc_out): Linear(in_features=600, out_features=600, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (fc_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (mha): MultiHeadAttention(\n",
       "          (fc_out): Linear(in_features=600, out_features=600, bias=False)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (fc_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ffwd): FeedForward(\n",
       "          (0): Linear(in_features=600, out_features=2400, bias=False)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=2400, out_features=600, bias=False)\n",
       "          (3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (norm1): LayerNorm((600,), eps=1e-05, elementwise_affine=True)\n",
       "        (normraw): LayerNorm((600,), eps=1e-05, elementwise_affine=True)\n",
       "        (normenc): LayerNorm((600,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((600,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=600, out_features=1210, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "model = Transformer(\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_head=6,\n",
    "    embed_size=600,\n",
    "    context_length=100,\n",
    "    dropout=0.1,\n",
    "    num_layers=6,\n",
    "    device=device,\n",
    ")\n",
    "model.load_state_dict(torch.load('saved_model_tokenizer_3.pth'))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([645]) tensor([399]) tensor([10])\n",
      "torch.Size([1, 100]) torch.Size([1, 1]) torch.Size([1, 1, 100]) torch.Size([1, 1, 1, 1])\n",
      "torch.int64 torch.int64 torch.bool torch.bool\n",
      "torch.Size([1, 1, 1210])\n",
      "tensor([179])\n",
      "tensor([645, 179])\n",
      "torch.Size([1, 100]) torch.Size([1, 2]) torch.Size([1, 1, 100]) torch.Size([1, 1, 2, 2])\n",
      "torch.int64 torch.int64 torch.bool torch.bool\n",
      "torch.Size([1, 2, 1210])\n",
      "tensor([315])\n",
      "tensor([645, 179, 315])\n",
      "torch.Size([1, 100]) torch.Size([1, 3]) torch.Size([1, 1, 100]) torch.Size([1, 1, 3, 3])\n",
      "torch.int64 torch.int64 torch.bool torch.bool\n",
      "torch.Size([1, 3, 1210])\n",
      "tensor([1207])\n",
      "tensor([ 645,  179,  315, 1207])\n",
      "torch.Size([1, 100]) torch.Size([1, 4]) torch.Size([1, 1, 100]) torch.Size([1, 1, 4, 4])\n",
      "torch.int64 torch.int64 torch.bool torch.bool\n",
      "torch.Size([1, 4, 1210])\n",
      "tensor([130])\n",
      "tensor([ 645,  179,  315, 1207,  130])\n",
      "torch.Size([1, 100]) torch.Size([1, 5]) torch.Size([1, 1, 100]) torch.Size([1, 1, 5, 5])\n",
      "torch.int64 torch.int64 torch.bool torch.bool\n",
      "torch.Size([1, 5, 1210])\n",
      "tensor([971])\n",
      "tensor([ 645,  179,  315, 1207,  130,  971])\n",
      "torch.Size([1, 100]) torch.Size([1, 6]) torch.Size([1, 1, 100]) torch.Size([1, 1, 6, 6])\n",
      "torch.int64 torch.int64 torch.bool torch.bool\n",
      "torch.Size([1, 6, 1210])\n",
      "tensor([91])\n",
      "tensor([ 645,  179,  315, 1207,  130,  971,   91])\n",
      "torch.Size([1, 100]) torch.Size([1, 7]) torch.Size([1, 1, 100]) torch.Size([1, 1, 7, 7])\n",
      "torch.int64 torch.int64 torch.bool torch.bool\n",
      "torch.Size([1, 7, 1210])\n",
      "tensor([648])\n",
      "tensor([ 645,  179,  315, 1207,  130,  971,   91,  648])\n",
      "torch.Size([1, 100]) torch.Size([1, 8]) torch.Size([1, 1, 100]) torch.Size([1, 1, 8, 8])\n",
      "torch.int64 torch.int64 torch.bool torch.bool\n",
      "torch.Size([1, 8, 1210])\n",
      "tensor([305])\n",
      "tensor([ 645,  179,  315, 1207,  130,  971,   91,  648,  305])\n",
      "torch.Size([1, 100]) torch.Size([1, 9]) torch.Size([1, 1, 100]) torch.Size([1, 1, 9, 9])\n",
      "torch.int64 torch.int64 torch.bool torch.bool\n",
      "torch.Size([1, 9, 1210])\n",
      "tensor([202])\n",
      "tensor([ 645,  179,  315, 1207,  130,  971,   91,  648,  305,  202])\n",
      "torch.Size([1, 100]) torch.Size([1, 10]) torch.Size([1, 1, 100]) torch.Size([1, 1, 10, 10])\n",
      "torch.int64 torch.int64 torch.bool torch.bool\n",
      "torch.Size([1, 10, 1210])\n",
      "tensor([614])\n",
      "tensor([ 645,  179,  315, 1207,  130,  971,   91,  648,  305,  202,  614])\n",
      "torch.Size([1, 100]) torch.Size([1, 11]) torch.Size([1, 1, 100]) torch.Size([1, 1, 11, 11])\n",
      "torch.int64 torch.int64 torch.bool torch.bool\n",
      "torch.Size([1, 11, 1210])\n",
      "tensor([399])\n",
      "tensor([ 645,  179,  315, 1207,  130,  971,   91,  648,  305,  202,  614,  399])\n",
      "<SOS> Je suis un prioc <EOS> \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def gen(model: nn.Module, sentence: str, max_len: int, vocab: BPE_Tokenizer, device: torch.device):\n",
    "    model.eval()\n",
    "    sos_token = torch.tensor(vocab.tokenize(\"<SOS>\"), dtype=torch.int64).to(device)\n",
    "    eos_token = torch.tensor(vocab.tokenize(\"<EOS>\"), dtype=torch.int64).to(device)\n",
    "    pad_token = torch.tensor(vocab.tokenize(\"<PAD>\"), dtype=torch.int64).to(device)\n",
    "    print(sos_token, eos_token, pad_token)\n",
    "    \n",
    "    src_input = torch.cat([sos_token, torch.tensor(vocab.tokenize(sentence), dtype=torch.int64).to(device), eos_token, pad_token.repeat(max_len - len(vocab.tokenize(sentence)) - 2)])\n",
    "    src_mask = (src_input != pad_token).unsqueeze(0).int() == 1\n",
    "    \n",
    "    tgt_input = sos_token\n",
    "    while tgt_input[-1] != eos_token and len(tgt_input) < max_len:\n",
    "        tgt_mask = dataset_train._causal_mask(tgt_input.shape[0]) == 1\n",
    "        src_input, tgt_input, src_mask, tgt_mask = src_input.to(device), tgt_input.to(device), src_mask.to(device), tgt_mask.to(device)\n",
    "        print(src_input.unsqueeze(0).shape, tgt_input.unsqueeze(0).shape, src_mask.unsqueeze(0).shape, tgt_mask.unsqueeze(0).shape)\n",
    "        print(src_input.unsqueeze(0).dtype, tgt_input.unsqueeze(0).dtype, src_mask.unsqueeze(0).dtype, tgt_mask.unsqueeze(0).dtype)\n",
    "\n",
    "        logits = model(src_input.unsqueeze(0), tgt_input.unsqueeze(0), src_mask.unsqueeze(0), tgt_mask.unsqueeze(0))\n",
    "        pred = F.softmax(logits, dim=-1)\n",
    "        print(pred.shape)\n",
    "        print(pred[:, -1, :].argmax(dim=-1))\n",
    "        # next_token = torch.multinomial(pred[:,-1,:], num_samples=1)\n",
    "        tgt_input = torch.cat([tgt_input, pred[:,-1,:].argmax(dim = -1).to(device)])\n",
    "        print(tgt_input)\n",
    "    print(vocab.detokenize(tgt_input.tolist()))\n",
    "gen(model, \"I am a student\", 100, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
